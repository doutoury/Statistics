
베타분포Beta dist. 와 감마분포Gm dist. 를 연결하는
아주 깔끔하고 유명한 예시가 하나 있다.
베타와 감마를 함께 얘기하기 좋은 시작점.

아직 하지 않은 한가지가 더 있는데,
베타분포Beta dist. 에서의 정규화변수Normalizing Constant	... 베이즈당구공 이야기에 해당하는 
							특수한 경우를 제외 하곤 아직 안 다룸
일반적으로 베타와 감마에서 사용할 수 있는 정규화변수를 찾아보자


# Beta dist. and Gamma dist. relationship

Example	: Bank-Post office
	( 은행-우체국 이야기 )

define some Notation.
	
	X~Gm(a, lmd) 	: 은행에 가서 X 분을 기다림	
	Y~Gm(b, lmd)	: 우체국에서 Y 분을 기다림
	they are independent mutually.


interpretation.				... by 24강 ### Relationship of Beta and Gamma

	- math.	a가 정수일 라면, 이걸 5 개의 독립항등분포인 lmd 를 지수로 하는 지수분포의 합으로 볼 수 있다

	- story.	이 가정은 마치 다섯 명의 사람이 줄 서 있고, 각 e^lmd 만큼 시간 동안 기다리는 것과 같다
		그 다섯명이 지나가길 기다리는 이야기


	
Find	About the distribution of your total waiting time T.
	1. X + Y		= T		... total waiting time
	2. X / (X+Y)	= W		... fraction of time


Case 1.	"If just find the one case, case 1., that's easy problem."
	
	If this 'a' and 'b' are integer,

		X~Gm(a, lmd)	is Sum of i.i.d. Exp(lmd) 지수분포 about a
		Y~Gm(b, lmd)	is Sum of i.i.d. Exp(lmd) 지수분포 about b
		So together this	is Sum of i.i.d. Exp(lmd) 지수분포 about a+b
	->	T~Gm(a+b, lmd)

	If this 'a' and 'b' are not integer,

		Just multiply MGFs 		... by MGF 적률생성함수
	->	T~Gm(a+b, lmd)



Case2.	"If I want to find distributions of those two things, not just of one thing. That's a 'Joint dist.' problem"

	1. X + Y		= T
	2. X / (X+Y)	= W

Find	Joint dist. of T and W.

	we knew 			the one 'marginal dist.' of T is Gm(a+b, lmd)	... by Case1. solution (???)
	we don't know yet 	another one 'marginal dist.' of W.
	we don't know yet 	those 'Joint dist.'



Think about it by yourself,
	Guess whether this total time T is independent or dependent of the fraction of the time W
		... 두 분포가 서로 독립적인지 아닌지 직관적으로 명확하지 않지? 
		... let's do calculate it

		수식적으로 볼 때 W 식에 T 가 포함되어 있으니까 독립적인 것 같기도 하고,
		이야기로 볼 때 총 대기시간이 은행에서 기다리는 시간의 비율과 무관하므로 독립인 것 같기도


Calculate.	
	Let	lmd = 1	to simplify notation. 		... by properties of Gamma dist.
							Rescailing : 일반성을 잃는 가정이 아니므로

			-> X 와 Y 에 대해 parameter lmd 를 1 로 바꿔도
			-> X / (X + Y) = T 인 확률변수 T 에는 문제 없다
			... rescailing 과정에서 분모 분자에 모두 /lmd 므로

find Joint PDF.
	f_(T, W) (t, w)	= f_(X, Y) (x, y) * |d(x, y) / d(t, w)|	... X, Y 의 Joint PDF 에 Jacobian 을 곱해서
							... by 19강 Joint dist.
							... multivariable 이랑 관련은 ... (?)
							... by 22강 Transformation (변수변환)
								식에서 Jacobian calculation
							-> 변수변환 중요 ! [ 22강 꼭 복습 ! ]

				* f_(X, Y) (x, y)	= f_X(x) * f_Y(y)			
							... X and Y are independent
							... by Joint dist. theorem of independet case

			= 1 / Gm(a) Gm(b) x^a e^(-x) y^b e^(-y) 1/xy * |Jacobian|

							... by Gamma dist.
							... by X, Y 가 independent 므로
							f_(X, Y) (x, y) = f_X(x) * f_Y(y)

				* Let's do the Jacobin calculation
							... I think it's little bit easier if we invert this
							and write down x, y in terms of t, w
							... 역수를 취해서 x, y 를 t, w 에 대한 식으로
				x+y = t	x/(x+y) = w 므로
				x = tw	y= t(1-w)

				|d(x,y) / d(t,w)| 는 x,y 의 t,w 에 대한 미분을 구하라는 의미
				자코비안 행렬식으로 적으면
				|d(x,y) / d(t,w)|	= | J |,
							| J | = 	| w	t	|
								| (1-w)	-t	|
							이 행렬의 행렬식determinant D 계산
							| D |	= |- tw - t(1-w)|
								= |- t|

								... 자코비안 행렬식에 절댓값 이유
								... 음의negative PDF 를 피하려고 !

			따라서,	|d(x, y) / d(t, w)| = |-t|	... by Jacobian calculation
							... 야코비안 판별식에 |절댓값| 을 취하는 이유
							음의 확률밀도함수 negtive PDF 원치 않으므로

			= 1 / Gm(a) Gm(b) x^a e^(-x) y^b e^(-y) 1/xy * t	, t > 0 이므로

	That is Joint PDF
	And the only thing we have to do is left to derive this equation in terms of t and w
	Becuase we still have x and y here.

	x = tw,	y = t(1-w)

			= 1 / Gm(a) Gm(b) w^(a-1) (1-w)^(b-1) t^(a+b) e^(-t) * 1/t

			w 에 대한 함수 w^(a-1) (1-w)^(b-1) 와 t 에 대한 함수 t^(a+b) e^(-t) 를 곱한 것
	f_(T, W) (t, w)	= f_T(t) * f_W(w)	꼴이 되었지 !!!

			-> therefore they are independent !	... 명확하진 않지만 독립임을 보였다 aproved
							... 각 t 와 w 가 어떤 분포를 따르는지 아직몰



	Next question is what's the PDF (of each t and w)

	우리는 위에서 X + Y = T 에 대한 dist. 가 Gm(a+b, lmd) 임을 확인했다.
	T~Gm(a+b, lmd)					... by MGF 적률생성함수
	
			= 1 / Gm(a) Gm(b) w^(a-1) (1-w)^(b-1) t^(a+b) e^(-t) * 1/t
				이 식에서 이상한 점은 정규화상수Normalizing Constant 가 없다는 점
				... ??? 	... 감마분포의 정규화상수 1/Gm(a+b) 말하는 건가 ?
					... 감마분포에도 정규화상수 라는 개념 있었나? 배타분포 아닌가?

				If you don't see the normalizing constant, make it by multiplying 1
				It means here multiplying Gm(a+b) / Gm(a+b)

			= Gm(a+b) / Gm(a) Gm(b) w^(a-1) (1-w)^(b-1) * t^(a+b) e^(-t) * 1/t * 1 / Gm(a+b)

	And then, 
	f_T(t)	= t^(a+b) e^(-t) * 1/t * 1 / Gm(a+b)		... that is the PDF of Gm(a+b, 1)



So, some conclusions of this,
1st.	T and W are independent.
2nd.	We have the one marginal PDF of T, about this Joint dist.


If we intergrate these things,
Let's get the marginal PDF of W.

	Remember to get the marginal PDF, we just intergrate (적분) the Joint PDF.
	Integrate f_(T, W) (t, w) with t (dt)
							... by 결합분포 (???) [ 결합분포 찾아보기 ]

Find.	f_W(w)	= Integral_inf f_(T, W) (t, w) dt

		= Integral_inf Gm(a+b) / Gm(a) Gm(b) w^(a-1) (1-w)^(b-1) * f_T(t) dt

			w에 대한 함수	: Gm(a+b) / Gm(a) Gm(b) w^(a-1) (1-w)^(b-1)
					-> t 에 대해 적분하면 상수 취급
			t에 대한 함수	: f_T(t)
					-> t 에 대해 적분하면, 감마분포 적분이므로 1

		= Gm(a+b) / Gm(a) Gm(b) * w^(a-1) (1-w)^(b-1)

There's a consequence of that,
We actually derived the normalizing constant of the Beta distribution. 

Normalizing Constant of Beta dist.

		in [ Gm(a+b) / Gm(a) Gm(b) * w^(a-1) (1-w)^(b-1) ] this equation,
		w^(a-1) (1-w)^(b-1)	is PDF of Beta(a, b)
		Gm(a+b) / Gm(a) Gm(b)	is constant (상수)
							... 만약 이 상수constant 가 틀릴 경우
							That would be not valid marginal PDF !

So,	f_W(w)	= {Gm(a+b) / Gm(a) Gm(b)} * {w^(a-1) (1-w)^(b-1)}



우리는 방금 전 이것이 marginal PDF 이고, 이 marginal PDF 가 적분intergrate 했을 때 1이 될 것임 보였다
따라서 {Gm(a+b) / Gm(a) Gm(b)} 이것이 베타분포Beta dist. 에 대한 정규화상수Normalizing Constant 임

		... by 베타분포의 정의
		Beta(a, b)		, a>0, b>0 
		PDF	f(x)	= c * x^(a-1) * (1-x)^(b-1)	, 0<x<1
		위의 식	f_W(w)	= {Gm(a+b) / Gm(a) Gm(b)} * {w^(a-1) (1-w)^(b-1)}

			-> 정규화상수 c = {Gm(a+b) / Gm(a) Gm(b)}

			... 이는 우리가 의도적으로 베타분포의 정규화상수를 구하고자 구한 것이 아니다.
			... 감마분포와 관련해 자연스럽게 떠올릴 수 있는 문제를 풀다가 우연히 marginal dist. 				가 Beta dist. 라는 것을 알았고, 여기서 베타분포의 정규화상수를 어쩌다 확인한 것 !
			(수학에서는 의도치 않게 다른 문제를 풀다 어떤 문제를 해결 or 발견하는 경우가 많다)



이 과정을 통해서 우리는 아래의 것을을 알아보았다.

	Now, we know	'the Normalizing Constant' of the Beta dist.
	and know	T~Gm(a+b, 1)
	and know	W~Beta(a, b)
	and know	they, T and W, are independent.


That's a very special 'properties of Gamma and Beta', about how they put together in that way.

여기서 이런 정리가 있는데,
There's a theorem.

	이 문제의 X~Gamma(a, lmd) 감마분포를 다른 확률분포로 대체하면, 
	이 X+Y=T 와 X/(X+Y)=W 는 더 이상 독립이 아닐 것이다.

	... 증명하기는 어렵지만, 어쨌든 '참' 인 정리theorem
	이는 감마분포와 베타분포의 특수한 성질을 보여준다.
	This is a special properties of Gamma and Beta.



이걸 어떻게 쓰는지 예제를 살펴보자

# Example of ~ this (?)

Find	E(W),
let	W~Beta(a, b) 

	there's two way to do it.

	(1)	One way is to use LOTUS (definition of Beta dist.)
		: 사실 LOTUS 라기보다, Beta 분포의 정의 (식) 이용
		... 기댓값의 정의에 따라 확률변수 w * PDF f 를 적분하면 다시 베타분포의 PDF 식이 나옴
		... 베타분포의 패턴 인식 문제

	(2)	There's a cloorer way to do it
		
		확률분포dist. 가 기댓값E 을 갖고 있다면, 그 기댓값E 은 단 하나만 갖는다.
		베타분포를 따르는 한 확률변수에 대해 어떤 기댓값을 얻었으면 
		다른 확률변수에 대해서는 다른 값을 얻어야 한다는 말

		따라서, 위 확률변수 W 가 따르는 확률분포가 Beta(a, b) 임이 변하지 않는다면
		어떤 방법을 사용하든 확률변수 W의 기댓값 E(W) 은 같게 나온다

		즉, 확률변수 W 가 일단 베타분포 Beta(a, b) 를 따른다고 하면,
		우리는 확률변수 W 에 대한 어떤 것을 구하든, 여러 가지 방법으로 접근할 수 있고,
		위에서 본 Case. W = X / (X+Y), X and Y are independent 를 가지고도 할 수 있다.
		(위에서 우리는 이 Case.에서 변수변환된 확률변수 W가 Beta(a, b) 분포 따름을 확인했으므로)

	using	E(X / (X+Y))	= E(X) / E(X+Y)
						... Not by Linearity !
						... This is the most common mistake in probability
						... 이 식은 확률에서 일반적으로는 거의 틀린 명제
						... This is very special case. be very careful!
						... 항상 이런 식은 성립하기 위한 '조건'을 기재해줘야 해

						" X~Gm(a, 1), Y~Gm(b, 1) " 이기 때문에
						" X / (X+Y) 와 (X+Y) 가 무상관uncorrelated " 이라고.

			* Why is [E(W / (X+Y)) * E(X + Y) = E(X)] in this special problem of Gamma-Beta ?
			(이 명제는 은행-우체국 문제, 즉 감마-베타 문제에서만 성립한다)
			
			Usually this proposition(명제) is wrrong.
			However, we have very important fact there, which T is independent of w.
			X / (X+Y) is independent of (X+Y).

			-> Therefore, They are 'uncorrelated (무상관)' 
				by definition of Uncorrelated
				E(XY)	= E(X) E(Y)

			X / (X+Y) and (X+Y) are uncorrelated, in this special case. So that's true.


	So,	E(X / (X+Y))	= E(X) / E(X+Y)
				= a / (a+b)	... by Gm(a, 1) 의 기댓값 = a, 
						... Gm(a+b, 1) 의 기댓값 = a+b


이렇게 Beta(a, b) 의 기댓값이 a / (a + b) 라는 것을 구하였다.

Similary, you can get the variance(분산) of Beta dist. or the variance of other functions of Beta.


Those are the main things of Gamma and Beta



# Order statistics	(순서통계량)

베타분포Beta dist. 와 순서통계량Order statistics 은 밀접한 연관이 있다.
(그 이유는 순서통계량을 다 살펴보기 전까진 이해하기 어려울 것)

What Order statistics are ?

	We already have seen some order strarisrics.
	In the homework, about " Covariance of the max. and the min. is not the Covariance of X and Y. "
	이 문제가 Order statistics 에 대한 직관을 갖게 해쥼.
	이 문제를 일반화genralize 한 경우가 바로 Order statistics.

Let	X_1, X_2, ... , X_n	be i.i.d.	(모두 독립항등분포를 따른다)

	순서통계량Order statistics 은 이 변수를 순서대로 놓는 것이다.
	It sounds pretty simple, But you really have to think carefuly what is actually means to do that.

Notation.
of the order statistics

	X_(1) <= X_(2) <= ... <= X_(n) , where	X_(1) = min(X_1, X_2, ... , X_n)
					X_(2) = sec.(X_1, X_2, ... , X_n)
					...
					X_(n) = max(X_1, X_2, ... , X_n)

					* 순서통계량의 특징
					- 중간값을 구할 수 있다.
						if n is odd.	the median = X_((n+1)/2)
						if n is even.	the median = 가운데 두 숫자 평균?
					- We can get other percentile (백분위수)
						quantiles		25%
						(사분위수)	75%
								...

					* 순서통계량의 중요성
					In statistics, a lot of times the data might come to 					independently in this particular order, 
					but you may not care about that.
					But you often may want the rank thing (순위), 
					like the largest, the smallest, the middle value, things like that.
					into quantile range (사분범위) it is a famous concept in statistics.

					All kinds of statistical quatities (통계적 변량) is defined
					in terms of 'Order Statistics'
					이렇게 통계적 변량은 순서통계량에서 정의됩니다	... (?)

						... come to		되다
						... particular		특정한	/까다로운
					 	... statistical quatities	통계적 변량
						( 통계에서 다루는 주요 변량등, 평균, 분산, 편차 등 ... )


		Let's think about the distribution 
		That's actually pretty hard problem. 
		The main reason that they're difficult to work is " since they are dipendnt "
		순서통계량의 분포를 구하는 것이 어려운 이유는, 순서통계량의 확률변수가 서로 종속이기 때문
		Eventhough, that's the key inside order statistics.
		순서통계량의 핵심 중 하나이기도 한 사실

		e.g. 
			- 순서통계량의 확률변수들이 양의 상관관계 (positively correlated)를 갖을 수 있다.
			eventhough 독립항등분포i.i.d.라는 가정이 있지만, they're still dependent 
			Maximum 은 항상 minimum 보다 크고, 3번째 숫자보다 7번째 숫자가 더 크다.
			they give an information each other. so we have to be very careful about that.

			-  Tricky in discrete casse.
			And also there are tricky in the discrete case, because of "ties"
			이산적인 경우에는 '동점ties' 을 주의해야 함. 두 확률변수의 값이 완전 일치하는 경우.
			That's serious issue in discrete case. 
			But in countinuous case, 두 확률변수가 무한한 소숫점까지 모두 같을 확률이 0이므로
			'동점ties' 에 대한 걱정 안해도 됨.


## Distribution of Order statistics,	in countinuous case.

Let	X_1, X_2, ... , X_n be i.i.d.	with PDF f, CDF F	(in countinuous)

Find	the CDF and PDF of X_(j)
					* 이는 이전 숙제에서 나온 최대 최소 문제와는 다름
					Maximum X_(n) 의 문제에서는, 최댓값이 10 보다 작다고 할 때,
					나머지 모든 변수들 X_(k)들도 10보다 작다. 그리고 후에							독립independent 을 이용하면 문제를 해결할 수 있다.

					* 이 문제에서는 X_(j)에 대한 일반적인 경우 general case 가 필요.
					This is going to be 'Marginal dist.' 주변분포가 될 것
					Since they're dependent (종속), 나중에 'Joint dist.' 결합분포도 봄

CDF	F	= P(X_(j) <= x)		... by def. of CDF

					*TIP. 	순서통계량을 다룰 때, 그림을 그리면 쉬워집니다
					수직선 상 어느 지점에 숫자 x 가 있는데, 이 사건 X_(j) <= x 을 
					그림의 입장에서 이해해보자.

					--|---------|----------|--------|-------->
					x_(1)	x_(j-1)	x_(j)	x

					
					1.	X_(j) 	는 x 보다 왼쪽에 위치할 것이고,
						X_(j-1)	는 X_(j) 보다 왼쪽에 위치할 것이고,
						...
						X_(1)	는 제일 왼쪽에 위치할 것이다.
					
					2.	X_(j+1)	는 X_(j) 와 x 사이에 있을 수도 있고,
							는 x 보다 오른쪽에 있을 수도 있다.
						X_(j+2)	는 X_(j) 와 x 사이에 있을 수도 있고,
							는 x 보다 오른쪽에 있을 수도 있다.
						...
						X_(n)	는 X_(j) 와 x 사이에 있을 수도 있고,
							는 x 보다 오른쪽에 있을 수도 있다.
					
					3.	이제 이 그림에서 알아본 사건을 수식으로 표현

	F	= P(X_(j) <= x)
		= P(at least j(개) of X_i's are <= x)

					... by intepreting in words
					" 순서통계량의 j 번째 변수가 x 보다 작거나 같다는 것 " 은
					" 확률변수 X_i 중 j 개가 x 보다 작거나 같다 " 는 것과 같다
					That's exactly same event.

			Let's write down a formula directly
			'at least j of X_i's' 에서 그럼 몇 개가 있는 건지 구체적으로 생각해보자
			let's suppose there're of them.

		= Sig_(k=j~n) cCk F(x)^k (1-F(x))^(n-k)

			위 사건 (at least j(개) of X_i's are <= x) 을 배반사건disjoint case 으로 쪼갠다
			x 의 왼쪽에 몇개가 있는지에 따른 배반사건으로. (j, n) 사이의 수 k 에 대해서.

			let's assume there are exactly k(개) left side of x.
			그런 일이 일어날 확률은 어떻게 될까?
			What's the distribution?
			이항분포 Binomial dist. 를 따른다 !

					성공	: x 의 왼쪽에 있는 것
					실패	: x 의 오른쪽에 있는 것
					으로 정의되는 이항분포.

			이 문제에서는 at least j 번의 성공의 경우를 말하고 있으니
			k 번의 성공에 대한 확률을 다 더하면 되겠지 ?	... (???)
			(k goes from j to n)

					Bin(k, p)
					p(성공)	: x의 왼쪽에 있는 확률	= F(x)


	So, that's the CDF
	CDF 를 구했으니, 이를 미분해서 PDF 를 구할 수 있겠지 ?

	하지만 이 식을 미분해서 구하면 big ugly sum 이 나올 거야
	Let's do the PDF at different way.

마찬가지로 그림을 그린 뒤 그 뜻을 헤아려보자

					          [dx]	
					-|--------	|----------|->
					 [ j -1 ]	x   [ n - j ]

					1.	In continuous case, x 에서의 X_(j)의 확률밀도 값(?) **
						= f(x) dx		, dx -> 0

					2.	X_(j) 는 총 n 개므로, n 가지 경우의수
						= n

					3.	나머지 j -1 개의 j 앞의 X_(i) 가 x 왼편에 있을 확률
						= (n-1)C(j-1) F(x)^(j-1) (1-F(x))^(n-j)
								... by 위의 CDF of orderstatistics

			
PDF	f_X_(j)(x) * dx	= n * (n-1)C(j-1) F(x)^(j-1) (1-F(x))^(n-j) * {f(x) dx}	... = 2. * 3. * 1.
	f_X_(j)(x)		= n * (n-1)C(j-1) F(x)^(j-1) (1-F(x))^(n-j) * f(x)

					**
					... by interpretation
					순서통계량의 j 번째 변수가 dx 라는 짦은 구간 안에 있을 확률
					The probability of that j's order statstics is in that tiny interval.

					Note.		... 이전 수업에서 다룬 내용
					여기서 밀도함수가 확률이 아니라는 사실을 유념.
					밀도함수는 1보다 클 수도 있다.

					If we Take a PDF and multiply by some tiny increament(증분),
					That's essentially the probability of that r.v. in tiny inteval length.
					무한히 작은 길이의 구간, length dx 을 곱해주고
					dx -> 0 으로 극한을 보내면, 그 값이 확률이 됨(?)

So,	That is the 'Marginal PDF'	(of order statistics ?)
	이렇게 주변 확률밀도함수를 구하였다


### Joint PDF of order statistics

그럼, 순서통계량의 결합분포의 PDF는 어떻게 될까?

What if you want the Joint PDF of the 3rd ortder statistics r.v. and the 7th order statistics r.v. ?
세번째 순서통계량 변수와 일곱번째 순서통계량 변수의 결합분포Joint dist. 는 어떻게 구하는 걸까?

	그림을 그리면 됩니다. (위처럼)
	두 개의 무한이 작은 구간이 있겠지? dx_1, dx_2
	그럼 비슷한 방법으로 결합분포joint dist. 를 구할 수 있다.
	(물론 다른 방법으로도 구할 수 있습니다)

					* Statistic 110 수업에서는, 
					순서통계량에서 주변분포marginal dist. 를 주로 다룬다. 
					종속이라는 걸 염두에 두면서.


### Example

The easist example to think about,
But very useful just to have in mind.

Uniform Order statistics	(균등분포의 순서통계량)

	U_1, U_2, ... , U_n	i.i.d.	~Unif(0, 1)
	U_1, 부터 U_n 까지 독립항등분포이고, 0부터 1사이에서 균등분포를 보이는 확률변수가 있다.

Find.	the distribution of the jth order statistics 
	j 번째 순서통계량 변수의 분포를 알고 싶다


j 번째 순서통계량의 확률밀도함수		... 위의 순서통계량의 PDF 식을 활용해 보자

					* 균등분포 Uniform dist.
					균등분포의 CDF 는 just increse linearily (선형증가)
					because PDF is constant (상수)
					CDF F	= x	... (?)
					PDF f	= 1	... (0, 1) 구간이므로. (otherwise, 0)

PDF	f_U_(j)(x)	= (n-1)C(j-1) x^(j-1) (1-x)^(n-j)	, for 0<x<1

					* Key part is
					... x^(j-1) (1-x)^(n-j)
					... 익숙한 꼴 pattern. This is Beta dist. (베타분포)
					... U_(j),	j 번째 순서통계량 변수는 Beta(j, n-j+1) 을 따른다 !

					U_(j) ~ Beta(j, n-j+1)

						상수 (n-1)C(j-1) 는 크게 중요하지 않다
						그저 적분intergrate 했을 때 1 로 만들어 주는 용도


					* 두 균등분포의 차이의 기댓값	... 이전 수업에서 다룸

					E(|U_1 - U_2|)	= 1/3

							|---------	|---------	|---------	|
							0	1/3	2/3	1

					차이의 절댓값은 곧, 최대 - 최소 이므로,
					E(|U_1 - U_2|)	= E(max. of U_1, U_2) - E(min. of U_1, U_2)

					위 예제에서 U_(j) 가 Beta 분포를 따른다는 관점에서 보면
					최댓값은 Beta(2, 1)을 따르고, 최솟값은 Beta(1, 2)를 따른다
					따라서 각각은 2/3 과 1/3 이 되기 때문에
							= 2/3 - 1/3
							= 1/3

							... 2-D LOTUS 를 쓰는 방법 대신 알아보았다
							... 균등분포의 차를 베타분포의 차이로 봄

							... 여기서 최댓값과 최솟값은 not independent
							... but, 선형성Linearity 은 항상 성립



마지막으로 여러분이 생각해보셨으면 하는 것 

Next big topic.	is 'Conditional Expectation'

In a sense you already know what 'Conditional Expectation' is, if you understand 'Conditional Probability', 
bucuase it just means to take expectation using 'Conditional Distribution' rather than using distribution.
만약 조건부확률 Conditional probability 을 알고있다면, 조건부기대 Conditional Expextation 역시 알 것.
because, 기댓값을 일반적인 분포 대신 조건부분포 conditional dist. 에서 구하는 것이기 때문.

For example.

A 라는 사건에 대해 X 의 기댓값		... A 가 주어졌을 때, 조건부 분포를 쓰는 것이라는 건 이해해야 함
					이미 친숙한 개념이지만, 그 특징에 관해 살펴볼 것
	E(X | A)

임의의 사건 A 에 대해

	E(X)	= E(X | A) P(A) + E(X | A^c) P(A^c)	로 표현 가능
						전체확률의법칙 과 비슷해 보이지 ?
						(확률 대신 기댓값을 쓴 것 뿐)
	In discrete case. 증명법
	E(X)	= Sig_x x P(X=x)			... by. def. Expected value
						Sum of (value * probability)

						... by Expand P(X=x) to two terms 
						with 전체확률의법칙
						두개의 항으로 나누면 위 정리 나옴
						(this is pretty intuitive formula) 

It's just expectation version of 'Law of Total Probability'




여기서 생각해볼 만한 퍼즐

Two Envelope Paradox	(두 봉투 역설)
... 몬티홀 문제보다 더 역설같아 보이는 문제)
[ 25강 마지막 부분 ]
문제랑 의문점 정리 - 


