
# LOTUS로 분산 구하기
						... 포아송분포의 모수를 람다라고 한다?
						-> X~Poiss(lmd)
						-> 그런데 포아송분포의 경우 평균과 분산의 값이
						 둘 다 모수인 lmd 를 따른다 ! 그래서 의미가 있즤

mean of the Poisson Distribution is 'lamda'
지난 시간에 포아송분포 평균(lamda)을 구하는 방법을 증명해 보았다.

이번 시간에는 포아송분포의 분산을 구하는 방법을 'LOTUS를 이용하여 증명'해 보겠다.
	... 포아송분포의 분산을 구하는 과정을 보면, LOTUS가 어떻게 성립하는지 알 수 있다.


일반적인 확률변수 X가 있다고 보자.
-> 확률변수 X_j=j 일 때의 확률을 P_j 라고 놓고 보자

PMF	PMF:	P_0,	P_1,	P_2,	P_3,	...
(확률질량함수)
확률변수	X_j :	0,	1,	2,	3,	...
	X^2 :	0^2,	1^2,	2^2,	3^2,	...

평균	E(X)	= Sig_x x * P(X=x)			... X는 이산확률변수이므로 E(X)는 x * PMF 의 합
	E(X^2)	= Sig_x x^2 * P(X=x)		... 

-> 이 이산확률분포에서 보면, X^2 이 3^2 이 될 확률은 P_3로 X가 3이 될 확률과 동일하다 !
	이게 LOTUS의 원리이고, 꽤 직관적으로 이해가 가능하다 !

-> 주의할 점은 이런 함수가 1대1 함수가 아닐 경우인데,
	예를 들어서 위의 이산확률변수X의 자료 중에 '음수' 가 있을 경우,
	확률변수X^2의 자료에 중복도는 같은 수의 자료가 생길 것. 응 근데 상관 없어. 생각해보면 상관 없음.
	(뭐 데이터 처리에서 중복되는 자료에 대한 sorting 작업 등이 추가되긴 하겠지만..)
	즉, LOTUS가 왜 작동되는지 알 수 있다. (이게 증명?!)
	LOTUS에 의하면 아무리 복잡한 함수라도 적용가능하다.

이제, 포아송분포의 분산을 알아보자
X~Pois(lmd) 일 때,
PMF	= {e^(-lmd) * lmd^k} / k!
E(X^2)	= Sig_0~inf k^2 * {e^(-lmd) * lmd^k} / k!		... by. LOTUS

	-> 이제 수열의 합을 구해야 하는데, 우리가 안쓰는 류의 수열의 합이네 ?
	-> 이런 수열을 푸는 방법	: 테일러 급수의 변형	... e^x 에 대한 테일러급수는 알제
	테일러급수를 변수 lamda에 대해서 써보자
테일러	=> Sig_0~inf lmd^k / k! = e^lmd	, 모든 lmd에 대해 성립 (복소수, 허수에 대해서도)
		-> 보통 Sig_inf (k) * lmd^k / k! 처럼 앞에 *k가 추가되는 경우에는 '양변 미분' 때림
		(e^x 함수의 미분 적분 성질)
	=> Sig_1~inf (k) * lmd^(k-1) / k! = e^k		... 양변 한번 미분 결과
							... k=0 일때 값은 0이므로 k=1부터와 같음
	=> Sig_1~inf (lmd) * (k) * lmd^(k-1) / k! = (lmd) * e^k	... 양변 미분해서 k^2 나오도록 양변 *(lmd)
	=> Sig_1~inf (k) * lmd^(k) / k! = (lmd) * e^k		... 이제 양변 두번째 미분 가능
	=> Sig_1~inf (k)^2 * lmd^(k-1) / k! = (lmd) * e^k + e^k	... 양변 두번째 미분 결과
	=> Sig_0~inf (k)^2 * lmd^(k) / k! = e^k * (lmd+1) * lmd	... k=0부터로 다시 바꿔주기 위해 양변 *lmd

다시 돌아와서
E(X^2)	= Sig_0~inf k^2 * {e^(-lmd) * lmd^k} / k!	= e^(-lmd) * e^lmd * (lmd+1) * lmd
						= lmd^2 + lmd

분산	Var(X)	= E(X^2) - E(X)^2	
		= lmd^2 + lmd - lmd^2	
		= lmd

"포아송분포(lmd)의 평균과 분산은 lmd로 같다"
	생각해 보면 뭔가 이상한 성질이다.
	차라리 규모가 비슷한 평균과 표준편차가 같다면 모르겠는데
	평균과 분산이 같다 ? 



# 정규분포에서 '표준화' 의 특성

Z <= (X-mu) / sg

정규분포에서 이 부분을 사용하기 좋은 이유는 
정규분포를 특정단위로 계산한 연속적인 측정이라고 생각하면 (길이, 시간, 질량 등의 어떤 단위이던.. )
임의의 단위로 X를 측정할 때, 
예를 들어 '초 sec'단위라면 정규화 과정에서 (X*sec - mu*sec) / sg*sec 의 분자 분모 sec 단위가 소거된다 !
즉, 정규분포의 확률변수Z는 '단위가 없는 크기' 라는 뜻 !!!

-> 따라서 표준화가 더 직관적으로 이해하고 해석하기 쉽게 느껴지고, 단위를 신경 쓰지 않고 비교 계산이 가능 !
	(예를 들어 초단위로 하나를 측정하고, 년단위로 다른 하나를 측정해도 둘 다 표준화 하면 같은 단위!)





# 이항분포의 분산 구하기
X~Bin(n, p) 를 따를 때, Var(x) 구하기				... 구하는 방법 3가지가 있는데
							01. 가장 쉽지만 아직 증명하지 않은 방식
							02. 쉬운 방식
							03. 번거롭지만 정직한 방식
03. 제일 정직하고 단연한 방법은
LOTUS를 사용해서 E(X^2)을 구하는 것
	-> 이건 하지 않을게, 위에서 계속 해왔으니까
	-> E(X^2) = Sig_inf 에다가 확률변수의 값 x^2 이랑, 이항분포의 확률질량함수 nCk*P^k 등.. 해주긔
	그리고 이 수열의 합을 구해주면 되겠지?
		근데 좀 tedious하니까 (수열의 합구하는 방법도 알아내야 하고, 많은 대수계산 필요..)

01. 가장 쉬운 방법은, 위에서 정리했지만 아직 증명하지 않은 정리

	Var(X+Y) is not equl to Var(X)+Var(Y)
		Variance is not LINEAR, unlike Mean
		분산은 평균갑과는 다르게 선형적이지 않다
	But if X, Y are independent, Var(X+Y)=Var(X)+Var(Y)	... 이 부분에 해당! 독립인 경우 !

우리는 이항분포를 배울 때,
이항분포는 n개의 Bernoulli 확률변수 p의 합이라는 것을 강조했다
따라서 위의 독립일 경우 적용되는 분산의 정리가 참이라면 사용 가능하다
	-> Bernoulli 확률변수 p의 분산만 구하면 된다
	-> Bernoulli 확률변수는 0 또는 1 이라서 계산하기 아주 쉽지 !
	-> Bernoulli 확률변수 p의 분산을 구한 뒤, n을 곱하면 이항분포의 분산이 나온다

02. 타협적 방법 (compromise mathod)				... 지금까지 배운것들의 연습이 되기 좋다
							... 특히 '지시확률변수'에 대해서
X를 Bernoulli 확률변수(p)에 대한 독립항등분포의 합으로 나타내보자
represent X as a sum of i.i.d. Bern(p)
X = I_1 + I_2 + ... + I_n,	I_j is i.i.d. Bern(p)			... I_n 으로 나타낸 것은 지시자임을 강조위해 			-> 이거 많이 해봤지?
			위 식은 j 번째에 성공하는지에 대한 지시자를 나타냄
			이것들을 더하면 이항분포를 얻을 수 있다

X^2 	= (I_1^2 + I_2^2 + ... + I_n^2) + 2*(I_1*I_2 + I_1*I_3 + ... + I_(n-1)*I_n)
	-> I_k 들은 i.i.d. (symmetry)	-> 교차항들 ; n개중에 2개 고르는 경우
E(X^2)	= n*E(I_1^2) + 2*nC2*E(I_1*I_2)			... by Symmetry
	-> I_1^2 = I_1	... I_k 는 지시확률변수로 0 또는 1	... by Indicator r.v.
	= np	+ n(n-1)*E(I_1*I_2)				... by Bernoulli(p)의 확률 = p

			I_1*I_2	; "indicator of success on 'both' trials 1, 2"
				-> Indicator r.v. of Indicator r.v. variation
			E(I_1*I_2)	; 따라서 이 기댓값은 이 사건이 발생할 확률
				-> 첫 번째와 두 번째 시도가 모두 성공할 확률
			E(I_1*I_2)	= p^2
				-> 두 번의 시도는 '독립적이므로' p의 제곱이 된다
	= np	+ n(n-1)*	p^2				... by Indicator r.v. (!!!)
	= np	+ n^2*p^2 - n*p^2
				->지금까지 계산한 부분은 이항분포의 '이차적률'

Var(X)	= (np + n^2*p^2 - n*p^2)	- (n*p)^2			... by 이항분포의 E(X) = np
	= np(1-p)
	= npq		, q = 1-p
				-> 따라서, 이항분포의 분산은 npq



# remind
	기하분포의 평균E(X) 계산 과정이 이 계산과정과 비슷했지 ?
	다만, Sigma에서 수열 계산 과정에서 e^x 에 대한 테일러 급수 대신 기하급수가 있던 것만 달랐다


# Hyper geometric (초기하)

베르누이 시행을 1회 할때 성공할 확률이 p이고 이 시행을 n번 반복한다고 하자.
확률 변수 X를 n번의 시행 중 성공한 횟수라고 한다면, X를 이항확률변수(Binomial Random Variable)라고 하며 모수가 (n,p)인 이항분포를 따른다고 한다.

이항분포는 시행 횟수와 관계 없이 확률이 일정하다는 전제를 근거로 두는 반면,
초기하 분포(Hypergeometric Distribution)는 모집단의 크기가 유한하고 실험을 수행하는 과정에서 모집단의 크기가 줄어드는2) 경우 활용할 수 있는 분포함수이다.

	... mid-term 이후에 초기하분포의 분산에 대해 다룰 예정


# LOTUS가 성립하는 이유 (증명)

이 증명은 선형성을 증명했던 것과 개념적으로 비슷
이것을 이산표본공간에 대해서만 증명하도록 하겠습니다
Prove LOTUS for discrerte sample space				... 이산표본공간

직관적 접근							-> 유한한 개수의 많은 자갈들
증명	-> show	E(g(X))	= Sig_x g(x) P(X=x)
				-> 여기서 P(X=x)로 확률질량함수PMF 사용할 수 있다
				(g(X)의 분포를 설명하기 위해 따로 계산해야 할 필요 없다)
			= Sig_s(in S) g(X(s)) P({s})			... 원소(자갈)와 확률(질량) 표현!!!
				-> s(in S)	; 각각의 s는 표본공간S에 포함되는 원소
				-> X(s)	; 확률변수는 '함수'이다 !
				(g(X)는 X에 대한 함수를 g에 대한 함수에 넣은 것 !)
				(*P({s}) 는 자갈s의 질량을 곱해주는 것)

		Sig_s(in S) g(X(s)) P({s})	; ungrouped
				-> 각각의 자갈들에 그 자갈들의 무게를 곱해서 
		Sig_x g(x) P(X=x)		; grouped
				-> 동일한 x값을 갖는 (S:X(s)=x인) 큰 자갈 x를 만듦
				(거대한 자갈은 같은 x값을 갖는 자갈들을 group화 한 것)
				(이 큰 자갈들은 g(x)값은 다르지만 X값은 같다 ... ???? 뭔소리야)
				-> 이것들을 묶어서 평균을 계산한 것이 위의 식

-> 사건과 표본공간, 사건의 확률(질량)에 대한 본질적 해석 방법 두가지 중 하나인 자갈과 질량에 대한 접근방식
중요하므로 이해 안되는 부분 다시 찾아보면서 생각 (표본공간에 대한 앞의 수업에서 한번 다뤘음. 조건부였나)
	
대수적 접근 (algebric)
증명		대수적으로 이해하고 싶으면 이것을 이중합으로 생각하면 됨
		-> x값들에 대한 합을 먼저 구하고,
		x값에 대한 각 합에 대해서 전체 자갈들의(s) 합을 구하는 것
		(X(s)=x 인 S에 대해서	-> S:X(x)=s 라고 표기)

		Sig_x g(x) P(X=x)	= Sig_x Sig_S:X(s)=x g(X(s)) P({s})
				= Sig_x g(x) Sig_S:X(s)=x P({s})	... X(s)=x 이므로
				-> g(x) Sig_S:X(s)=x P({s}) 는 'x라고 표시된 모든 자갈들 질량의 합'
				(이 부분이 앞 직관적 접근에서 말한 '거대한 자갈'의 질량)
			 	= Sig_x g(x) P(X=x)
				-> X=x 가 의미하는 것은 ?	'사건' (that's a event)
				S:X(s)=x 가 일어나는 사건을 의미
				

-> 이 내용은 [사건과 확률변수]에 대한 아주 기본적인 내용 !!!	(확률통계 수업 앞 부분의 기본 중요!)
이게 왜 LOTUS 가 성립하는지에 대한 이유 (결국 사건과 확률변수에 대한 기본적인 이유임)
