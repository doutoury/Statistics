
지난시간에
연속확률변수 중에서는 유일하게 지수분포exponential dist.가 '무기억성memoryless'을 갖는다고 배웠다
(= 연속시간continuous time)
이산확률변수 중에서도 비슷하게 기하분포geometric dist.가 '무기억성memoryless'를 갖는다
(= 이산시간discrete time)

In a very deep sense, 
한편으로는 기하분포가 지수부포의 이산 유사형태이고, 지수분포가 기하분포의 연속 유사형태이다
-> 두 분포의 관계가 매우 깊음을 알 수 있다

	
	얼마전 본 기사
	기대수명(life expectations)의 뜻을 사람들이 완전히 잘못 이해하고 있다
	Basically 기댓값expectation과 조건부 기댓값conditional expectation의 차이를 잘 이해하지 못한 실수
	
	내일태어날 아기들의 기대수명을 어떻게 알 수 있을까?
	태어난 아기들을 죽을 때까지 관찰한 후 이를 평균내어야 하는데.. 
	그럼 이전까지 살았던 사람들의 기대수명은 어떨까?
	지금까지 죽은 사람들의 나이를 바탕으로 평균을 내어야 하는데.. 아직 살아있는 사람은 빠졌네..?

	이러한 예시를 '중도절단자료' (censored data)라고 한다		... 어려운 통계학적 문제
	
	이 기사에서 잘못된 점은
	여기서 구한 평균 기대수명을 '모든 사람에게 평균 기대수명이 80세다'라고 하는 것
	이미 50세, 60세인 사람에게도 기대수명이 80세라고 가정하는데
	사실은 너가 더 오래 살수록 수명이 더 길어질 것은 기대한다
		-> 이 사실을 방적식으로 표현해보자
		T ; 얼마나 오래살 것인지에 대한 r.v.,	최소 20년 이상 살 수 있다는 조건하에 기대수명은 ?
		-> 이런 조건(정보)가 주어질 때, 기댓값을 계산하는데 비조건부확률 대신 조건부확률을 사용
		따라서 이는
		E(T | T>20) > E(T)	일 것이다
			(만약 모든사람이 같은 수명을 갖는다면, 평균기대수명은 이 조건(정보)와는 무관)
			(수명은 편차가 있기 떄문에 T>20 이라는 조건이 좋은 소식. 즉 유관하다는 것)
		인간의 일생은 무기억성이 아니다	(사람은 시간이 갈수록 쇠퇴한다)
			이를 통해 무기억성이 무엇인지 이해해보자
			인간의 일생이 무기억성이고, 평균수명이 80세일 때,
			당신이 20살까지 살았다면 그 순간 새로운 기대수명은 100살이 된다
			(무기억성에 의하면 언제나 새롭게 시작하기 때문)
		If memoryless we had,
		E(T | T>20) = 20 + E(T)
		-> That's what the memoryless would say.

'무기억성memoryless'은 인간에게 현실적이지도 않은데 왜 알아야 되나여?
01. 우선, 각종 과학 연구에 쓰임. (화학, 물리 등 여러 응용에), 경제학과 같은 분야에서도 쓰이고
'시간이 지나면서 쇠퇴하지 않는' 경우에는 꽤 현실적이다.

경우에 따라서는 그렇게 비현실적이지도 않다. 꽤 '합리적인 근사치'가 될 수 있다. 비록 완전히 맞진 않더라도.

ex) 숙제를 할 때, 단계적으로 계산을 하면서 해결해 나가는 문제가 있고, 내내 고민하다가 갑자기 떠올라서 해결해내는 문제가 있다. 전자는 partial progress가 가능하고, 후자는 partial progress가 불가능. 해결이 안될 수도 있다.

02. 다른 것들을 위한 기초가 된다
가장 유명하고 실질적으로 많이 쓰이는 '와이블분포'의 경우
와이블분포는 지수분포의 거듭제곱형태 인데, 
지수확률변수를 세 제곱하면 더는 지수분포도 아니고 무기억성도 없다.
그래도 아주 유용하게 변한다. 
지수분포는 아주 중요한 기초. (응용을 위한)


## '오직' 지수분포만 무기억성을 갖는 이유에 대한 증명

하나의 정리theorem 

만약 X가 양의 연속확률변수이고, 무기억성을 갖고 있다면 그 분포는 반드시 지수분포를 따른다.
If X is positive continuous r.v. with memoryless property, then X~Expo(lmd) for some lmd.

	... this is a characterization of the exponential distribution.
	...무기억성은 '분포에 대한 특징'이지, 확률변수의 특징은 아니다 !

증명

이전까지의 흔히 보이는 증명과 좀 다르게 접근할 것이다.
방정식을 쓸 거지만 변수를 구하는 것이 아니라 함수를 구하는 것이기 때문. 이를 함수방정식이라고 한다.
because, We're gonna write down an equation, but we're solving fuction, not solving variables. It's called fuctional equation.

Let F be the CDF of X, 
G(x) 	= P(X>x) 			... 이런 계산은 G(x)로 놓고 푸는게 편하다
	= 1 - F(x)

but 지난시간에 지수분포를 구할 때 한 것 처럼, 이런 계산에서는 1-CDF 로 두고 푸는 것이 훨씬 쉽다.
T 시간보다 길게 생존할 확률이기 때문

memoryless property is
G(s_t) = G(s)*G(t)			... 저번 시간에 했던 것
				무기억성은 조건부확률에 대해서 정의할 수 있다. 이 식처럼.

-> 여기서 보여주고 싶은 것은, 평소 방정식 풀 듯이 G 함수에 s와 t에 대해 푸는 것이 아니라 G에 대해서 푸는 것
오직 지수함수G만 이런 성질을 만족한다는 것을 보이고 싶은 것이다


이런 방정식에 접근하는 방법은 여러 가지를 대입해 G에 대해 조금씩 알아가는 것이다
G에 대해서 풀어봅시다

모든 양수에 대해서 G(s_t) = G(s)*G(t) 가 성립하는지 보아야 하지?

한가지 할 수 있는 건 s=t로 놓고 풀어보는 것
s=t 라면,	G(2t) = G(t)^2,
	G(3t) = G(t)^3,
	...
	G(kt) = G(t)^k, k>0 integer	... 귀납적으로 알 수 있다


G(2t)말고 G(t/2)를 알고 싶다면 어떻게 할까?
위의 귀납식에 t 대신 t/2를 대입하면 G(t) = G(t/2)^2 이고, 양변을 1/2제곱하면
	G(t/2) = G(t)^(1/2)
	G(t/3) = G(t)^(1/3)
	...
	G(t/k) = G(t)^(1/k)

이제 우리는 위 식이 k가 양의 정수일 때와 양의 정수의 역수일 때 성립한 다는 것을 알아 내었다 (귀납적으로)
이제 k가 유리수일 때를 알아내 보자

유리수의 정의는 '두 정수의 비율' 이다. -> 모든 유리수를 m/n으로 표현 (m과 n은 integer)
	G(m/n * t) = G(t)^(m/n)	... 위의 두 성질을 이용! (물론 여기서는 양의 실수가 되겠지)

이제 실수를 보면,
실수는 '유리수의 극한' 이라고 볼 수 있다. (like Pi)		... ??? 불규칙적 무한대여도?
	-> "유리수에 극한을 씌우면 실수를 얻을수 있다."
* 코시수열을 이용한 '유리수의 극한은 실수' (by Cantor)
https://kin.naver.com/qna/detail.nhn?d1id=11&dirId=1113&docId=60672415&qb=7Jyg66as7IiY7J2YIOq3ue2VnCDsi6TsiJg=&enc=utf8&section=kin&rank=1&search_sort=0&spq=0&pid=U6rFqwp0YiRss5xKFBZssssssmK-196933&sid=IxjpaTw2IW3usZzAuT/FXg%3D%3D

	lim_inf G(m/n * t) 	= lim_inf G(t)^(m/n)
	so,	G(x * t) 	= G (t)^x		, for any real number x>0 (모든실수 x>0)

	그럼 이 식은 모든실수x와 모든양의실수 t에 대해 성립하므로 t=1일 때를 보자 	...(?)
	t=1 ...	G(x)	= G(1)^x			... 지수함수처럼 보이네?
			=  e^x*(ln G(1))		... 밑이 e인 지수함수로 표현해 보자
			-> G(1)은 위에서 정의한 대로 '확률'이다. 0<G(1)<1 이므로 'ln G(1) < 0' 음수 !
			= e^x*(-lmd)		... ln G(1) = - lmd 로 표현해 보자 (lmd > 0)
			-> 'ln G(1)' ... 이부분은 '상수'다. 특히 음의 상수. 따라서 -lmd 라고 표현 (lmd>0)
			
			= e^(-lmd)*x
			-> 이건 결국 [1-지수분포]의 누적분포함수

	-> 따라서, 무기억성이 성립하는 '확률밀도함수'는 지수분포가 유일 !
	(지수분포가 유일한 무기억성 분포)




이 시점에 많이 필요한 함수가 있다
(평소에 많이 쓰인다. 처음에는 이상할 수 있는데, 무슨 뜻인지 깊이 생각해보면 왜 유용하고 실제로 어떤 의미인지 알 수 있을 것)





	적률생성함수에 대한 보충설명
	https://blog.naver.com/ableyd/221322125705
	https://en.wikipedia.org/wiki/Moment-generating_function
		라플라스변환	(about Probability theory)
		https://en.wikipedia.org/wiki/Laplace_transform
		-> 라플라스변환공식은 통계에서는 평균을 정의하는 식과 같다.


# Moment Generating function (MGF)	적률생성함수
누적분포함수CDF나 확률밀도함수PDF처럼 MGF는 '분포를 설명하는 또 다른 방법'

정의	r.v. X has MGF M(t) = E(e^(tx)), as a function of 't', if this is finite on some (-a, a), a>0
	확률변수 X의 MGF는 M(t) = E(e^(tx)) 다. 't'에 관한 함수. 구간 (-a, a), a>0 에서 유한한 경우
								(... on a space of finite measure)

	-> 0 주위로 어떤 범위 내에서 '유효하지 않은 이상'(?) 유용한 개념은 아니다.	... ???
	모든 실수 범위 내에서 유한할 수도 있는데, 그건 좋은 상황일 경우고
	그 정도로 요구하지는 않는다. 적어도 0 주위의 어떤 작은구간 안에서라도 '유한하면' 된다

	이 정의가 뜬금없이 나타난 것 같죠 ?
	또 누군가는 대체 t 가 뭔지 궁금해 한다. t 가 무엇을 의미 하는가 ?
	
	우선, t 는 그저 '가변수'(dummy variable)다.	... t is just a placeholder. (그저 빈 칸 채우기 용)
	-> MGF 식의 미분을 통해 n차 적률을 구할 때, t=0을 넣어 테일러급수의 나머지 항들 날리기 위한 (?)

	두 번째로 대체 MGF가 무슨 뜻일까 ? 
	일전에 '확률변수의 함수도 확률변수'라고 했었지 ? 
	여기서 MGF 또한 확률변수X에 대한 함수로 정의되므로 MGF도 '확률변수'이다.
	So, 기댓값E을 구할 수 있다 !			... 왜 굳이 구하는지는 몰라도 구할 수 있음은 알 수있
	
	-> MGF는 모든 실수 t에 관한 함수다. 기댓값을 구할 수 있으니(?) 이는 잘 정의된 t에 대한 함수다 !
	
	t 를 부기(book keeping)하기 위한 수단이라고 생각
	모든 MGF는 분포(dist.)의 적률(moment)을 추적하기 위해 부기하는 독특한 수단이다
	
Why is it called Moment 'Generatig' ?

MGF	M(t)	= E(e^(t*x))			... e 위에 지수가 있으니까, 테일러급수로 전개해 보자
		= E(Sig_0~inf (x^n * t^n) / n!)	... e^x 의 테일러 급수는 언제나 수렴
						... 따라서 이 식 변환conversion 성립
						(x는 확률변수지만 항상 수렴한다)

		-> 이 꼴을 보고 intuitively, E 와 Sigma 순서를 바꿔 계산하고 싶어지지 ?
		-> 이 부분에서 좀 기술적인 조건이 필요, 특히 구간 (-a, a)에 대해서.
		If 평균계산과 합계산의 순서변환이 가능하다고 가정하면

		= Sig_0~inf (E(x^n) * t^n) / n!	-> 테일러급수 안에 n차적률이 들어가 있는 꼴 !

		... 이는 꼭 무한대 버전의 선형성(linearity) 같지 ?

-> 이게 적률생성함수 MGF = E(e^(t*x))가 n차적률 E(x^n)과 관련이 있다는 증명!

만약 유한급수(finite sum)였다면, 선형선(linearity)에 의해 sum과 sigma의 교환법칙(commutative law)이 성립가능하다는 것을 바로 알 수 있다. but, 무한급수(infinite sum)라면 더 증명해야 한다.
이를 증명하려면 해석학(certain real analysis)이나 통계학210 수업을 들어야 하므로 증명은 생략.


1차적률은 확률분포의 '평균' 이었고,
2차적률은 확률분포의 '분산' 을 구하는 데 필요했지.
n차적률, 더 높을 찻의 적률은 또 다른 해석이 가능하고 평균과 분산보다 더 복잡하다. 하지만 많은 이유로 유용!

근데 보통 평균과 분산에 관심이 있지, 더 높을 차원의 적률에 관심이 없다.
그럼 왜 MGF가 중요한지 세 가지 이유를 알려쥼

## Why is MGF important ?
Let X have MGF M(t)

1st reason	적률(momnet)을 구하기 위해		... 가끔 정말로 적률 E(x^n)을 구해야 할 때 !
		The n'th moment, E(x^n) is	 i) coefficient of t^n / n! in Taylor series of M (about 0)
						...0에서의 테일러긊의 t^n / n! 의 계수
					ii) n'th derivative (0)	... 기초미적분에서 배움
						... n차 미분을 하고 t=0을 대입한 값
						(=> 0을 대입한 MGF를 n번 미분하면 n차적률값 나옴)

2nd reason	MGF가 '분포를 결정'하기 때문	
			... 만약 서로다른 두 확률변수 X, Y가 있고, 둘이 같은 MGF 갖으면 둘의 분포는 같다
			(즉, 같은 CDF누적분포함수나 연속분포라면 같은 PDF확률질량함수 를 갖는다!)
		MGF determines the distribution, 
			... If X, Y have same MGF, then they have same CDF
		이 부분은 증명하기 매우 어렵기 때문에 증명하지는 않으나, 알면 유용하다.
			-> 위키피디아 MGF definiton 부분 보면, 
			Bilateral Laplace 변환식에 함수f(x)가 들어가 있는데,
			이 함수 부분을 Probability theory에서 확률밀도함수 CDF f(x)도 같게 취급 가능 ?

3rd reason	(확률변수의?) 합을 구하기 매우 쉬워진다
			-> 일반적으로 독립적인 확률변수의 합의 분포를 구하는 것은 굉장히 복잡하다 !
			-> '합성곱'이라고 한다	(합성곱 합이나 합성곱 적분은 복잡해)
		근데 만약 MGF를 안다고 하면, 
			예를 들어 X의 MGF가 M_x, Y의 MGF가 M_y 이고, X와 Y가 서로 독립적이면
			두 변수의 합 X+Y의 MGF는 각 변수의 MGF의 곱과 같다
		E(e^(x+y)) = E(e^x) * E(e^y)
		[ 서로 독립적인 두 확률변수의 곱의 기댓값은 각 기댓값의 곱과 같다 ]
			... 아직 증명하진 않았지만 (독립성이 중요!)
			... X와 Y가 독립적이기 때문에, E(t*x)와 E(t*y)도 독립적이다!
			-> (변수의 함수는 변수이기 때문에 ?)
			... 이에 따라 위의 정리 성립
			= M_x(t) * M_y(t)

		-> 즉, 두 확률변수의 MGF를 모두 안다면, 두 MGF를 곱하면 '확률변수의 합'을 알 수 있다.



# Couple of examples about MGF
특정 분포의 MGF를 풀어보자

## MGF of Bernoulli dist.
시작하기 가자 좋은 분포는 Bernoulli			... Bernoulli 는 가장 쉽고 간단한 분포

X~Bern(p),
	M(t)	= E(e^tx)
		= p*e^t + q	, q=1-p		... 베르누이 분포의 확률변수X는 0 또는 1이기 때문에
						e^tx 는 e^t 거나 1이다. 두가지 경우 밖에 없는데
						베르누이 분포의 확률변수의 두 개의 가능한 값의
						가중평균(waighted average)을 구한 것.
		
## MGF of Binomial dist
이에 따라서 바로 이항분포의 MGF를 구할 수 있게 되었네, 
정의대로 쓰게 된다면 LOTUS를 써야하는데, 
이항분포를 '독립적이며 동일한 분포를 따르는 베르누이 p의 합' (i.i.d. Bernoulli p)으로 생각하고 
위의 적률생성함수MGF의 세번째 정리를 이용하면 바로 이항분포의 MGF를 Bern의 MGF를 통해 알 수 있다.

		... X와 Y가 서로 독립적이면, 두 변수의 합 X+Y의 MGF는 각 변수의 MGF의 곱과 같다
		E(e^(x+y)) = E(e^x) + E(e^y)
		[ 서로 독립적인 두 확률변수의 곱의 기댓값은 각 기댓값의 곱과 같다 ]

X~Bin(n, p)
	M(t)	= (p*e^t + q)^n,	q=1-p


## MGF of Normal dist.
*Notice	표준정규분포의 MGF를 알게 되면 그 어떤 정규분포의 MGF도 알 수 있다 !
location and scale 에 대한 정리. (m+sg*Z 를 통해 다른 정규분포들의 MGF를 구할 수 있게 된다) 

표준정규분포의 MGF 먼저 확인해 보자. 단순하니까.

Z~N(0, 1)
	M(t)	= Integral_-inf~inf e^tz * [1/root_2pi * e^{-(z^2)/2} dz	... 평균의 정의에 따라
		= 1/root_2pi * Integral_-inf~inf e^{tz - (z^2)/2} dz	... by LOTUS
		= 1/root_2pi * Integral_-inf~inf e^{-1/2 (z-t)^2} dz * e^{(t^2)/2}
								... 완전제곱꼴로 고침
								... z에 대한 1차항 없어서
								대수적 풀이 (적분)하기 수월
		= e^{(t^2)/2} * Integral_-inf~inf 1/root_2pi * e^{-1/2 (z-t)^2} dz
								... 표준정규분포를 t 만큼 평행이동									한 정규분포의 CDF 적분하는 식!
								... 따라서 적분값은 1 나옴
		= e^{(t^2)/2}

-> 여기서부터 해서 표준이 아닌 정규분포도 구할 수 있다. 해봐야 함. ㅇ ㅅㅇ

MGF에 대해서 나중에 다시 더 다뤄보고, 사용해보고




MGF와 상관 없지만
유명한 확률 예시 하나	(Famous probability exmple)


		## 조건부확률밀도함수 (Conditional probability density duction)	
								... 라플라스룰 전에 익힐 개념
		X(몸무게)에 대한 Conditional PDF를 구하고 나면, 각 키(Y=y)에 따른(주어졌을 때) 
		몸무게(X)의 분포를 알 수 있다. 즉, X의 conditional PDF는 x의 함수이다. 정의는 아래와 같다.
	
		when (X, Y)~f(x, y), given Y=y
		conditional PDF of X is
		f(x | y) = f(x, y) / f(y)	, f(y) > 0
	
		기호로는 X | y 또는 X | Y=y ~ f(x | y) 로 나타낸다.

	

# Laplace's Rule of Succession	라플라스의 후속규칙
오래된 유명한 문제이다
"내일 해가 뜰 확률"

만약 최근 연속 n일 동안 해가 떴다면 내일 해가 뜰 확률은 얼마일까 ?
the assumption 가정이
확률변수 	X_1, X_2, ... ,	i.i.d. Bern(p)
독립적이며 동일한 분포를 따르는 베르누이 p	... ( Bern(p) or Bern(1, p) )

확률변수 X_n을 지표로 봐바. n째 날에 해가 뜰 확률 등의 지표. (확률변수X_n 을 Indicator 로 보자)

	-> 이 문제가 실제로 해가 뜰지 않뜰지에 대한 문제로 보면 쓸모 없는 문제 같아보이지만
	실제로 '이러한 구조'의 문제는 굉장히 유용하다 !

라플라스는 '해가 뜰 확률은 미지수' 라고 합니다. 즉, p를 알 수 없다는 말.
문제는 알 수 없는 p를 어떻게 해결해야 할까 ?

좀 더 정확하게 말하자면 p가 주어졌을 때, 위의 명제는 사실이다.
Given p,	X_1, X_2, ... ,	i.i.d. Bern(p)	... 참 (?)

p를 안다면 위를 가정해도 된다고 합시다
모든 확률변수 X_1, X_2, ... 는 '조건부독립'이다.	Conditional Independent

하지만, 지금은 모름
	해가 n번 연속 떴다는 증거는 있지만, p의 값은 알 수 없다
	따라서 p를 미지수라고 하자	('p' is unknown)

가장 철학적으로 심오한 통계학 논쟁 중 하나는 '어떻게 미지수를 처리할까'
수십년 동안 베이지안과 빈도학파 사이의 논쟁이 있었다.		... 111 수업 과정의 큰 주제
문제는 'p'가 미지수(unknow)인데 어떻게 처리할 것인가에 대한 것

	Bayesian	베이지안의 입장
	미지수이기 때문에, 불확실성을 '어떤 분포를 띄는 확률변수'로 보아 계량해야 한다고 봄
	treat 'p' as a random variable
		-> 분포distribution는 결국 불확실성uncertainty을 나타냄
		베이지안이라 불리는 이유
		'베이즈의 정리'를 이용해 모든 증거가 주어질 때 p의 분포가 무엇인지 구할 수 있기 때문
			-> p의 사전지식으로 시작해서, 곧 어떤 자료data나 증거가 있기 전이라
			사전의 불확실성이 있다는 뜻. 자료를 수집했을 때 베이즈 정리를 이용함.
				-> Bayes rule 베이즈의 정리는 '증거를 통해 갱신' 하는 방법
				(갱신하면 새로운 불확실성이 생기겠지 ... ???)
	약간 이상하게 느껴질 수 있다
	원래 베르누이 확률분포를 갖는 확률변수X가 일어날 확률인 'p'를 '확률변수'로 보는 것이 안 익숙해서.
	하지만 이렇게 보는 연습할 필요가 있다. 확률변수가 무엇을 의미하는지 깊이 생각해보게 해준다.

	그럼 이제 p를 확률변수라 놓고
	라플라스laplace에 의하면 p를 균등분포라 하면	(꼭 균등분포일 필요는 없다)
	Let p~Unif(0, 1)	... 사전확률 (prior)		... 그럼 왜 균등분포uniform dist.라 했는가?
						... 라플라스에 의하면 균등분포는 완전한 불확실성을
						반영해야 한다고 함	(논쟁의 여지는 있다)
						-> completely random, we know nothing about 'p'
	and let 	S_n	= X_1 + X_2 + ... + X_n
	이라고 가정하고, 이 문제의 구조(structure)는
	S_n | p~Bin(n, p)	, p~Unif(0, 1)		... p~Unif(0, 1)	: Prior distribution 사전분포
						... S_n | p		: Likelyhood	우도 ...(???)
			given p is Binomal. this is the condition of p. 
			... 'p를 아는 상수로 놓는다' 는 뜻
			우리는 거의 매번 이렇게 해 왔지. 동전 던지기 1/2 처럼. but, 항상 그런 것은 아냐.
			임의 동전 문제와 같은 경우. 	(-> 독립과 조건부독립의 차이를 묻는 문제였던)
			'만약 어떤 동전이 될 지 안다면'
			= '만약 해가 뜨는 확률을 안다면'
			여기서 가정하는 것은 '독립적이며 같은 분포를 따른다' (i.i.d.) 는 것이고
			독립적이며 같은 분포를 따르는 Bernoulli p 의 합은 이항분포 np 라는 것을 앎.
			
			하지만 p는 균등분포unif 의 확률변수다
			라는 이러한 구조를 갖는다.

	1st. 이제 문제는 우선 '사후분포posterior dist.'를 구하는 것.
		정의	posterior distribution
			자료data를 수집한 후의 분포를 뜻함
	p | S_n					... Posterior distribution 사후분포

	S_n을 관찰할 수 있다고 가정할 때,	X_1 ~ X_n 까지 관찰한다고 가정하는 것, 결국 같은 결과 나옴
					이를 충분통계량Sufficient statistic 이라 함. (111 수업의 주제)
					X_1 ~ X_n 중 몇개가 1인지 관찰하는 것만으로도 충분하다 함.
					-> 따라서 S_n을 조건condition을 놓아도 되고,
					X_1 ~ X_n 까지를 조건으로 놓아도 상관없다.
	find posterior distribution	p | S_n
	
	2nd. 그 다음 문제는 더 실질적으로 '내일 해가 뜰 확률'이 무엇인지 구하는 것 (우리가 구하고자 하는 것)

	and find '내일 해가 뜰 확률'	P(X_n+1=1 | S_n=n)
				... "지난 n일 동안 해가 떴을 때, 내일 해가 뜰 확률" 의 수식


How to solve this problem ?		... Bayes Rule
익숙하지 않은 형태지만 베이즈 정리 형태와 완전히 유사하다.
define this posterior distribution	... 베이즈의 정리 bayes rule 와 거의 비슷한 것을 써보도록 하겠다

	f(p) 를 구해보자		... f 는 일반적으로 확률밀도함수CDF를 뜻함
		자료가 있기 전에는 균등분포uniform 로 봤지만
		자료가 있고 나서는 '일정 밀도를 가지기 때문에' 확률밀도함수를 가지지만 '조건이 있다'
				-> 조건부 확률밀도함수
	f(p | S_n=k)	
		k=n 일 때를 볼꺼지? , (지난 n일 동안 해가 떴을 때)
			->이를 일반적으로 말하면, 
		"지난 n일 중 k일 동안 해가 떴을 때, p에 대한 불확실성을 어떻게 갱신update할까?"
			-> 이건 just 베이즈 룰. (전에는 본 적 없는 형태)
			한 편으로 이 부분f(~)은 확률밀도함수PDF 인데,
			확률밀도함수는 확률이 아니지만, 직관적으로 확률이라고 생각할 수 있고,
			적어도 확률밀도함수에 작은 증분을 곱하면 대략 그 작은 구간의 확률이 된다. (???)
			-> CDF 함수의 적분값이 확률인데, 근사값으로 CDF 함수값의 작은 사각형 너비 ??
			이 경우 베이즈 정리는 확률이 아닌 밀도를 무시한 것과 비슷하게 생겼다
			... ??? 뭔소리야
			-> 확률과 확률밀도를 같이 취급하고 계산하겠다는 건가?

	그래서 p 와 S_n=k 두 부분을 바꾸겠다
	[ P(A | B)		= P(B | A) * P(A) / P(B) ]	... 조건부확률 정리는 이건데 ...
	[ P(H | E)		= P(E | H) * P(H) / P(E) ]	... 베이즈룰	
						... 베이즈룰은 가설 H와 증거 E의 조건부확률의 관계
						... 가설 H의 확률에 증거 E가 얼마나 영향을 주는가
						... 구하고자 하는 것은 가설 H의 확률 P(H) ... (?)
							P(H | E)	: 사후확률 posterior probability
							P(E | H)	: 우도 likelyhood Probability
							P(H)	: 사전확률 prior probability
							P(E)	: 주변우도 marginal likelyhood

	f(p | S_n=k)	= P(S_n=k | p) * f(p) / P(S_n=k)

			f(p) = 1 	; 사전확률분포의 CDF인데, 균등분포uniform dist. 이기 때문에 '1'
			P(S_n=k)	; 주변우도의 확률분포인데, p에 종속되지 않으므로 (p에 대해 독립) '상수'
			(분자는 p에 대한 함수지만 분모는 p에 종속하지 않으므로 분모는 조건부가 아님!)
						... 이를 제대로 증명하기 위해서는 전체확률의법칙으로
			= 상수 * p^k * (1-p)^(n-k)

	k=n 일 때,
	f(p | S_n=n)	= 상수 * (n+1) * p^n

			the assumption 가정이
			확률변수 	X_1, X_2, ... ,	i.i.d. Bern(p)
			독립적이며 동일한 분포를 따르는 베르누이 p	... ( Bern(p) or Bern(1, p) )
			이므로 P(X_k=1) = p 이다 ?!

	P(X_n+1=1 | S_n=n)
			연속적인 전체확률의법칙 (Law of total probability)
			전체확률의법칙의 연속적인 버전
			
	P(S_n=k)	= Integral_0~1 P(S_n=k | p) * f(p) dp
			P(S_n=k)의 확률을 구하고자 하는데, 주어진 자료가 '연속확률분포' f(p)이기 때문에,
			p가 주어졌을 때의(조건부) S_n=k 사건이 일어날 확률을 CDF f(p)의 적분으로 구함!
				(참고? : https://tkchen.wordpress.com/tag/law-of-total-probability/ )
				(참고? : 9강 fundamental bridge)
			= Integal_0~1 p * 상수 * (n+1) * p^n dp	... ???
			= (n+1) / (n+2)




*Note	f(p | S_n=k)	이 표기는 익숙해지기 쉽지 않다
			통계학에서 보통 확률변수와 값을 구분하려고 소문자와 대문자를 구분해 사용하는데
			여기서는 p 가 소문자면서도 확률변수기 때문
		(그럼에도 소문자 p를 쓰는 이유는 확률변수라고 할지라도 값을 아는 상수를 의미하기 때문)

