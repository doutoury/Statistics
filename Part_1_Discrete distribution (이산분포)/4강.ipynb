{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4강ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyOm5FDHDyUX7G6AurWccPC9"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"T9Gb2Bwu2mWK","colab_type":"text"},"source":["# 4강 <br>\n","\n","  - ### __Independence__ <br>\n","  - ### __Pairwise Independence__ <br>\n","  - ### __Newton-Pepys Problem__ <br>\n","  - ### __Conditional probability__ <br>\n","  - ### __Bayes’ Theorem__\n","\n","\n","<br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"1S10322330iK","colab_type":"text"},"source":["Today, I want to introduce __Independence__. <br>\n","We've already been talking informally about independence. when we say things are independent. <br><br>\n"," And this big main topic for this entire week is __Conditional probability__ and __conditioning__.\n","\n"," <br><br><br>\n"," "]},{"cell_type":"markdown","metadata":{"id":"Bcbbr5JJ3ap9","colab_type":"text"},"source":["## Independence <br><br>\n","\n","We already have __some intuition__ about what does it mean to have independent events. That is like one of them gives you __no infromation__ about the other one, basically. <br><br>\n","\n","But that definition is too vague to actually verify if these events are independent or not. <br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"P56ua_OY_Y4G","colab_type":"text"},"source":["### Definition of Independence <br><br>\n","\n","\n","It's very easy to define. <br>\n","It's not so easy to understand fully the definition. \n","\n","<br><br><br>\n","\n","\n","#### Def. Independence &emsp; (for 2 events) <br><br>\n","\n",">when there are 2 events A and B  <br><br>\n",">\n",">If $P(A \\cap B) = P(A) P(B)$, <br><br>\n",">\n",">they are __independent__. <br>\n",">In other words, they have __nothing__ to do with __each other__.\n","\n","&emsp; &emsp; (there are some equivalent forms we'll see later) \n","\n","<br><br><br>\n","\n","\n","\n","#### Def. Independence &emsp; (for 3 events) <br><br>\n","\n",">When there are 3 events A and B and C <br><br>\n",">\n",">If $P(A \\cap B) = P(A) P(B)$, <br>\n",">&ensp; $P(A \\cap C) = P(A) P(C)$, <br>\n",">&ensp; $P(B \\cap C) = P(A) P(B)$, <br>\n",">&ensp; $P(A \\cap B \\cap C) = P(A) P(B) P(C)$ <br><br>\n",">\n",">they are __independent__. <br><br>\n",">\n",">We just need pairwise independences, and we also need all three of them at once too. <br>\n",">In general, we can't get rid of any of these conditions and we can't say that the one of these will imply the others.\n","\n","<br><br><br>\n","\n","\n","\n","#### Def. Independence &emsp; (for more than 2 events) <br><br>\n","\n","Similarly for $n$ events, $A_1, \\cdots, A_n$ <br>\n","It's completely analogous with 3 events case <br><br>\n","\n",">Any 2 of these are independent, <br>\n",">Any 3 of these are independent, <br>\n",">Any 4 of these are independent, <br>\n",">... <br>\n",">Any n of these are independent, <br><br>\n",">\n",">then these $n$ events, $A_1, A_2, ..., A_n$ are independent. \n","\n","<br><br>\n","\n","\n","The basic rule is __\"Independent means multiply when we're trying to find the probability of an intersection\"__. <br><br>\n","\n","This is just kind of a slogan. This is not a math statement. This slogan is just paraphrasing the definition of independent. It's saying, if all these multiplication rules hold, that's the definition of independence. <br><br>\n","\n","You can also think about the naive definition of probability and your naive intuition of independence. Put those together and see whether it's consistent with this. It will be. \n","\n","<br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"sUATAQ3mISnR","colab_type":"text"},"source":["#### Note : meaning of $P(A \\cap B)$ <br><br>\n","\n","__Mathematically__ <br>\n",">$P(A \\cap B)$ means __the intersection__\n","\n","<br>\n","\n","__Intuitively__ <br>\n",">$P(A \\cap B)$ means what's the probability that __both A and B occur__.\n","\n","\n","<br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"h2rw2wMQ_4Bo","colab_type":"text"},"source":["#### Note : Disjoint <br><br>\n","\n","If A occur, then B cannot occur, &emsp; when events A and B are disjoint. <br><br>\n","\n","Contrast __independence__ with __disjointness__. <br><br>\n","\n",">\"A and B are __disjoint__\" that means that if A occured then B cannot possibly occur. (That's opposite to independence) <br>\n",">\n",">Independence says, if we know that A occurs, it tells us nothing whatsoever about whether B occurs. <br><br>\n",">\n",">So they're completely different concepts. \n","\n","<br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"ccHC4z9_6zjP","colab_type":"text"},"source":["#### Example : Newton-Pepys propblem <br><br><br>\n","\n","\n","\n","#### Problem <br><br>\n","\n","Having 6-sided fair dice, numbered 1 through 6 all equally likely, <br>\n","The question is, which of these is most likely of these 3 possibilities? <br><br>\n","\n","A. at least one $6$ with six dice. <br>\n","B. at least two $6$ with twelve dice. <br>\n","C. at least three $6$ with eightteen dice. <br><br><br>\n","\n","\n","&emsp; &emsp; ... Pepys thought C is most likly, but the truth was A.\n","\n","<br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"LphxJovTBYS2","colab_type":"text"},"source":["#### Solution <br><br>\n","\n","At tihs point, this problem should be pretty easy calculation for us. <br>\n","  - You can use __the naive def. of prob__. because all six outcomes are 'equally likely'. \n","  - You can use using __independence__ instead. \n","\n","<br>\n","\n","Assuming that the rolls of the dice are independent. Either you roll one dice stimes, twelve times, or eightteen times independently. The every dice are independent of each other intuitively. The intuition corresponds to __the definition of independence__. <br><br><br>\n","\n","\n","As far as out strategy, whenever we see at least one possibility, that shout make you think of __a union__. <br><br>\n","\n","  - When you see a union, one approaches __inclusion_exclusion__. <br>\n","  - But on the simpler way, if I do __the complement__, __the union__ will be __the intersection__. \n","\n","<br>\n","\n","And in this case, we're takinng __an intersection__ of independent stuff, so that it means we just __multiply__. <br><br><br>\n","\n","\n","#### Complement, Intersection, Independence <br><br>\n","\n","$P(A) = 1 - (\\frac {5}{6})^6$ <br>\n","&emsp; &emsp; &ensp; $\\approx 0.665$ &emsp; &emsp; &emsp; ... counter-intuitive ! <br>\n","\n",">It's $1 - P(all \\; of \\; the \\; dice \\; are \\; non \\; 6s)$. <br><br>\n",">\n",">For individual dice, there's 5/6 chance that it's not a 6. and Independence says we can just multiply those. This agrees too in terms of the naive def. of prob.\n","\n","<br>\n","\n","$P(B) = 1 - (\\frac {5}{6})^{12} - 12 (\\frac {1}{6})(\\frac {5}{6})^{11}$ <br>\n","&emsp; &emsp; &ensp; $\\approx 0.619$ &emsp; &emsp; &emsp; ... 5% less likely then A.<br>\n","\n",">It's $1 - P(all \\; of \\; the \\; dice \\; are \\; non \\; 6s)$ $- P(one \\; 6s \\; and \\; the \\; others \\; elevens \\; non \\; 6s)$. <br><br>\n",">\n",">We're going to do the complement, but here we have to think a little bit more. <br>\n",">In the case of $P(one \\; 6s \\; and \\; 11s \\; non \\; 6s)$, We have to do a specific way that suppose to label the dice 1 through 12. there, Dice number 1 has a 6, and the other 11 dice labeled are non-6s. But that was just one possibility. The 6 could have been any of the twelve dice, so we just have to multiply this by 12. \n","\n","<br>\n","\n","$P(C) = 1 - \\displaystyle \\sum_{k=0}^{2} \\binom {18}{k} (\\frac{1}{6})^{k} (\\frac{5}{6})^{18-k}$ <br>\n","&emsp; &emsp; &ensp; $\\approx 0.597$ &emsp; &emsp; &emsp; ... 5% less likely then A.<br>\n","\n",">It's $1 - P(all \\; of \\; the \\; dice \\; are \\; non \\; 6s)$ $- P(one \\; 6s \\; and \\; 11s \\; non \\; 6s)$. <br><br>\n",">\n",">I'm going to subtract off the possibilities where there are zero dice 6s, one dice 6s, or two dice 6s cases. And write this as a sum. In there, $ \\binom {18}{k}$ is the number of ways to choose where the $6$s are. And then fore the $6$s, it's 1/6 to the $k$. And then for the non-$6$es, it's 5/6 to the $18-k$. <br>\n",">(It is called 'binomial probability', we're going to be seeing a lot of these later)\n","\n","<br><br><br>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"59FSkBwA3z3a","colab_type":"text"},"source":["#### Answer <br><br>\n","\n","So that means A is the most likely, <br>\n","and C is actually the least likely. <br><br>\n","\n",">So Newton basically did the same calculation and sent it back. This was in the very very early days of probability. <br><br>\n",">\n",">But, Newton's argument is only correct when all the dice are equally likely. if you change the probabilities of faces of the dice, this result is not. So the newton's argument is invariant. \n","\n","<br><br><br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"Be0mdPbg32ZP","colab_type":"text"},"source":["## Conditional Probability <br><br>\n","\n","\" How should you update your probability /beliefs /uncertainty when you recieve new evidence? \" <br><br>\n","\n","You have to update your uncertainties based on new evidence. <br><br>\n","\n","It's generally a sequential process. <br><br>\n","\n",">In practice, you live each day of your life, and you learn new things every day. I said on the very first day of class that statistics is the study of uncertainty. you have uncertainty about a lot of things (in your whole life), but you keep learning every day. At least, that's what I consider the ideal. That's a very generic situation. <br><br>\n",">\n",">The question is, how should you update your uncertainty, update your beliefs, and update your probabilities, based on new evidence? <br>\n",">You can see why that's a central question in science, and in philosophy, and just in thinking. <br><br>\n",">\n",">It's generally a sequential process. Because you may have some probabilities today, and you learn things later today, and you update your probabilities. And then you'll learn more stuff tomorrow, and you update again. So your old probabilities get updated to new ones, bur then the new one becomes the old one you update that. And you keep updating and updating, hopefully, in a coherent, consistent, logical manner. <br><br>\n",">\n",">So we're going to develop the mathematicala and statistical ideas of how do you do that.\n","\n","<br><br>\n","\n","\n","You can see this is a very deep and very broad question. And, in fact, I like to say that \"conditioning is the sould of statistics\". <br>\n","Everything relates to conditioning in one way or another. So now I've set this up as something very important. <br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cp8QP1xLRBTi","colab_type":"text"},"source":["### Mathmatical expression : Conditioning <br><br>\n","\n","\n","Let's express conditioning mathematically, not sounds philosophical like above. <br><br><br>\n","\n","\n","\n","#### Definition <br><br>\n","\n","$P(A|B) = \\frac {P(A \\cap B)}{P(B)}$ &emsp; &emsp; &emsp; &emsp; &emsp; , if &emsp; $P(B) > 0$ <br><br>\n","\n","$A|B$ &emsp; : this is the notation that we have one event $A$ given another event $B$. And pronouce that \"$A$ occurs given that $B$ occurs\". <br><br>\n","\n","Interpretation : <br>\n","Initailly, we had some probability, $P(A)$ for $A$ occurring. Initially, we don't know whether $B$ is going to occur or not. then suppose that we observe that, \"Oh, $B$ auctually did indeed occur\". <br>\n","Now if $A$ and $B$ are independent, then that's going to be irrelevant. But if $A$ and $B$ are not independent, then that's valuable information. And the question is \"how do we update our probability for $A$?\" <br><br>\n","\n","That's a very simple looking definition. It's just $P(A \\cap B)$ over $P(B)$. And it sounds like this very complicated, general, deep, subject like above, but it reduces to just a fraction. <br><br>\n","\n","We're going to prove some theorems. actually, many theorems. But each theorem is so easy, that it may not seem like how can this be useful. But then we're going to be seeing examples over and over again for the rest of the semester why this is a useful concept. \n","\n","\n","<br><br><br>\n","\n","\n","#### Intuition <br><br>\n","\n","I'll explaing this def. of conditional probability in two ways. <br><br><br>\n","\n","\n","#### Intuition 1 : pebble world <br><br>\n","\n","  - get rid of events (pebbles) in $B^c$\n","  - __renotmalize__ to make mass 1 again\n","\n","<br>\n","\n",">$S[\\; B (\\cdot \\; \\cdot \\; \\cdot \\; A ( \\cdot ) \\cdot \\; \\cdot \\; \\cdot) \\cdot \\; \\cdot \\;] $ <br><br>\n",">\n",">We've moved __beyond the naive definition of probability__. So we no longer assume that all outcomes are equally likely. <br><br>\n",">\n",">Thinking there's a sample space S. and let's assume that there are a finitely many possible outcomes. Each one is represented by a pebble, $[\\; \\cdot \\;]$. Just for the sake of example, dawing nine pebbles. When we talk about sample space, Elements are all possible outcomes of our experiment. So in this example, there are nine possible outcomes. <br><br>\n",">\n","> The constraint (assumption) on the pebbles is that __the total mass equals $1$__. Each pebble has a certain weight or mass. So each one might be that they're all 1/9, or they might be different probabilities, like some of the pebbles are more massive than others. <br>\n",">(we can always define the units of mass so that the total is one. So we're assuming that we are using units where the total mass is $1$) <br><br><br>\n",">\n",">\n",">Now, we've talked about the correspondence between probabilities, between events or subsets. __An event is just a subset__. Therefore an event is a set of pebbles in this case. <br><br>\n",">\n",">Let's suppose that $B ( \\; \\cdot \\; \\cdot \\; \\cdot \\; \\cdot \\;)$ this is event $B$. <br><br>\n",">\n",">And we want to do $P(A|B)$. Think about what is that mean. <br><br>\n",">\n",">First of all, the interpretation is that we learned that $B$ occurred. Therefore, all the pebbles that are outside of $B$, that's the other five of them, are now irrelevant. Because they didn't occur. We know that one of these ones in $B$ occurred. <br><br>\n",">\n",">So all we have to do is __get rid of pebbles in $B$ compliment__, in other words, pebbles that are not in $B$. Anything in $B$ complement is now completely __irrelevant__. <br>\n",">We are restricting to the part of the space that did occur, and we don't care about the part of the space that didn't occur. <br><br>\n",">\n",">Now we just have these 4 pebbles, that was in $B$. That's our world now. We're thinking of the sample space is the univrse, but our universe got restricted to $B$. That's what we're doing. <br><br>\n",">\n",">We're just going to do the usual laws of probability, everything the same as before. If we want the probability of $A$, look like this $A(\\; \\cdot \\; \\cdot \\; \\cdot \\; \\cdot \\;)$. Three of the pebbles in $A$ are outside of $B$. Those are just irrelevant now. The only part that matters is the part that's only one pebble of $A$ in $B$. That's what this is, $P(A \\cap B)$. <br><br><br>\n",">\n",">__Renomalization !__ <br><br>\n",">\n",">__Why do we divide by $P(B)$?__ The reason is an obvious technical difficulty. But I said, get rid of these pebbles that are not in $B$, and then just do what we were doing before. The only problem is, when we got rid of those, __these ones remaining don't have total mass $1$__. So all we do is __\"renomarlize\"__. <br><br>\n",">\n",">Renormalizing is just that multiply by a constant so that the new total mass equals $1$. So here, $\\frac {P(A \\cap B)}{P(B)}$ this is the correct normalization. if you divide by $P(B)$, you are going to make the total mass $1$ again. The reason is just that, if we let $A = B$, that $\\frac {P(A \\cap B)}{P(B)}$ is $\\frac {P(B)}{P(B)}$. And that is $1$.  \n","\n","<br>\n","\n","That's the end. So this seems like an intuitive thing to do. We get rid of the part of the space that was irrelevant, and then retrict to that part, and we need it to add up to 1. That's all we're doing. That's what that definition is. \n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gSzmd5tRBmew","colab_type":"text"},"source":["There are different perspectives on what doese probability really mean. <br><br>\n","\n","In pebble world, we just __do the experiment once__, and we play around with the pebbles, and we see what is the rational probabilites to use. <br>\n","In frequentist world, we don't just do the experiment once. We __repeat the experiment over and over again__, many times. <br><br>\n","\n","One philosophical objection to this is, how many is many? Is it infinitely many? Is it a million times? Is it possible to step into the same river twice? The ancient Greek asked that, because you've changed it after the first time you step in. So can you do the same experiment over and over again? <br><br>\n","\n","That's a deep philocophical question, but let's just assume you can, do the same experiment over and over again. And here's what we do. <br>\n","\n","\n","<br><br><br>\n","\n","\n","#### Intuition2 : frequentist world <br><br>\n","\n","Assuming that we can, indeed, repeat exactly the same __experiment over and over again__, then one __interpretation of probability__ is __long-run frequency__. That's a pretty intuitive - if you flipped a coin 1,000 times, and 612 of them were head, then you might say that probability is about $\\frac {612}{1000}$. So you're interpreting probability as, in the long-run, what fraction of the time does the event occur if you repeat the experiment over and over again. <br><br>\n","\n","So now all we need to do is make a list of all the repetitions. You do the experiment. Let's suppose we're just generating some binary data for example. It doesn't actually matter. I'm just making up some numbers here. <br><br>\n","\n","$(100101101)$ <br>\n","$\\;\\,001001011\\;$ <br>\n","$(111111111)$ <br>\n","&emsp; &emsp; &ensp; $\\vdots$ <br><br>\n","\n","We're repeating over and over again. We're generating lots of data. <br><br>\n","\n","  - Circle the repetitions where B occured. <br>(Repeating many many times, only select the ones where B occurred) <br>\n","  - Among those, what fraction of time did A also occur? <br><br>\n","\n","So that should be a pretty intuitive way to think of it. Because we're just thinking that we should restrict our attention to just the circled experiments where B occurred, because we're given that B occurred. Among those, what fraction of time did A occur? <br><br>\n","\n","If you think about this intuition a little more, you can also see why this def. of conditional prob. $P(A|B) = \\frac {P(A \\cap B)}{P(B)}$ corresponds to this.\n","\n","<br><br><br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"UCyDw0TFDAch","colab_type":"text"},"source":["### Some theorems <br><br>\n","\n","There are some theorems to use in the case of the conditional probability. \n","\n","<br><br><br>\n","\n","\n","\n","#### Theorem 1 : the probability of $A \\cap B$ <br><br>\n","\n","Find the probability of __A intersect B, $P(A \\cap B)$__. How would we go about that? take this equation of the def. of conditional prob. $P(A|B) = \\frac {P(A \\cap B)}{P(B)}$. And I want $P(A \\cap B)$, so I would multiply both sides by $P(B)$. <br><br>\n","\n","And we could have used $P(B|A)$ instead of $P(A|B)$. But $P(A \\cap B)$ is the same thing as $P(B \\cap A)$. So therefore, we can derive two theotems here at the same time about __how to get the probability of an intersection__. <br><br>\n","\n","\n",">$P(A \\cap B) = P(B)P(A|B)$ <br><br>\n",">\n",">$P(B)P(A|B) = P(A)P(B|A)$\n","\n","<br><br>\n","\n","\n","#### Notice : independent case <br><br>\n","\n","Notice that if A and B are independent. remember the definition of independent? $P(A \\cap B) = P(B)P(A)$. <br><br>\n","\n","So independent is the case where $P(A|B) = P(A)$. That says intuitively - that says __\" conditioning on B does nothing \"__. <br><br>\n","\n",">If A and B are independent, $P(A|B) = P(A)$\n","\n","<br><br><br>\n","\n","\n","\n","#### Theorem 2 : the probability of $A_1 \\cap A_2 \\cap \\cdots \\cap A_n$ <br><br>\n","\n","General form of theorem 1. Let me just generalize this theorem 1 to $n$ of them, $A_1$ through $A_n$. I want to know what's __the probability that all of these events $A_1, A_2, ..., A_n$ occur__. <br><br>\n","\n","Just applying this theorem 1 multiple times. formally, you can use induction, but I'm just using this repeatedly. <br><br>\n","\n",">$P(A_1 \\cap A_2 \\cap \\cdots \\cap A_n)$ <br><br>\n",">\n",">$= P(A_1 \\; P(A_2|A_1) \\; P(A_3|A_1 \\cap A_2) \\; \\cdots \\; P(A_n|A_1 \\cap A_2 \\cap \\cdots \\cap A_{n-1})$ \n","\n","<br>\n","\n","Actually, this is not just one theorem, but __$n$ factorial theorems__. So we are extremely efficient today. <br>\n","It's $n$ factorial, because I could have started with A_7. As long as you're adding a new one each time, conditional on all the previous ones. Hopefully, you all see the pattern here. There are $n$ factorail ways to do it. <br><br>\n","\n","For some problrms, if you try to do it in one order, it's going to be really hard. And if you try a different permutation, it'll be really easy. So it is useful to think about these different orderings. (???) \n","\n","<br><br><br>\n","\n","\n","\n","#### Theorem 3 : Bayes' rule - relation between P(A|B) and P(B|A)<br><br>\n","\n","\n","I want P(A|B) in terms of P(B|A), So let's divide both sides of theorem 1 by P(B). <br><br>\n","\n",">$P(A|B) = \\frac {P(B|A)P(A)}{P(B)}$ \n","\n","<br>\n","\n","This theorem is called __Bayes' rule__. it was discovered in 1760 - something by Bayes. <br><br>\n","\n","The proof is completely obcvious, just dividing by P(B). But __the implications are extremely deep__. There's whole field of statistics, __Bayesian Statistics__. Controversies have raged for centuries just about this kind of thing. As I wrote it here, it's uncontroversial. But the question of how to use it and how to think about it is very deep. So this is __one of the most useful theorems__ you will ever see. And it's __just easy algebra__. "]}]}