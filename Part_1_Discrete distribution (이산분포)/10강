{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"10강","provenance":[],"private_outputs":true,"collapsed_sections":["aETlyOJ_Xfmj","-spQNxvpQl1m","U6mFqtZKv44-","SRZ22_I1-3gO","zYJOW0k-6CFk","LtEpAe03uzNx","GhncvrhmCotS","zxKEl-giH_nI","kzyWIqetTK65"],"authorship_tag":"ABX9TyPcin8ewMsHsH/9sFFkitNd"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"tzVumfxO1oxv"},"source":["# 10강 <br>\n","\n","  - ### Linearity <br><br>\n","\n","  - ### Negative Binomial Distribution <br><br>\n","\n","  - ### First Success Distribution\n","\n","\n","<br><br><br>"]},{"cell_type":"markdown","metadata":{"id":"l8CyRAyl4n7N"},"source":["## Linearity <br><br>\n","\n","\n","### Proof of Linearity $\\quad$ ( Linearity of Expectation ) <br><br>\n","\n","\n",">It's my own proof. <br>\n",">Remember the definition of Expectation we wrote last time. <br>\n",">One of that is the naive way just adding numbers up and divide by how many numbers there are, <br>\n",">The other way is we grouped the same numbers together and take the weighted average. <br>\n",">$\\rightarrow \\quad$ That's basically the proof of Linearity.\n","\n","<br>\n","\n","Let $T = X + Y$, <br>\n","Show $\\mathbb{E}(T) = \\mathbb{E}(X) + \\mathbb{E}(Y)$ <br><br>\n","\n",">This proposition(?) is pretty obvious if $X$ and $Y$ are independent. <br>\n",">But, This is not at all obvious to most people if $X$ and $Y$ are dependent !\n","\n",">This is always true as long as these expectations exist. <br>\n",">( Even if they're dependent )\n","\n","<br>\n","\n","Assuming the discrete case. <br>\n","The proof is analogous in the continuous case. <br><br>\n","\n","$t$ is a possible value of $T$, and $P(T=t)$ is the PMF. <br>\n","So this is the weighted average. <br><br>\n","\n","$\\displaystyle \\sum_t t P(T=t) \\quad \\stackrel{?}{=} \\quad \\sum_x x P(X=x) + \\sum_y y P(Y=y)$ <br><br><br>\n","\n","\n","\n","\" How do we deal with this ? \" <br><br>\n","\n","We can try for the left-side term to split in two parts, <br>\n","or for the right-side term to be combined into one. <br><br>\n","\n","But we don't know how to combine a sum over x and a sum over y into one, <br>\n","so let's consider the alternative approach. the left-side term. <br>\n","This is a little bit more promising. <br><br><br>\n","\n","\n","\n","Remember, <br>\n","$\\qquad$ Our most important strategy is conditioning ! <br>\n","$\\Rightarrow \\quad$ Wishful thinking ! <br><br>\n","\n","\" What do we wish we knew ? \" <br><br>\n","\n","Well, it would be useful to know $X$ ( or to know $Y$ ). <br>\n","So, let's condition on $X$ ! <br><br>\n","\n","So I can write this as ... <br>\n","$P(T=t) \\quad = \\quad \\displaystyle \\sum_x P(T=t|X=x)P(X=x)$ <br><br>\n","\n",">Here, $T$ is $T = X+Y$. <br>\n",">So, if $X$ and $Y$ are independent, we can solve this. <br>\n",">But, if $X$ and $Y$ are dependent, we stuck with this given $(X=x)$ forever ...\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"aETlyOJ_Xfmj"},"source":["### Pebble world <br><br>\n","\n","\n","Let's do this from another point of view.\n","Just by looking at Pebble world. <br><br>\n","\n","<img src=\"https://drive.google.com/uc?id=1NdafPrrSPy7iLc3Wxd-Lak4pFMiD8yug\" alt=\"10-01\" width=\"300\">\n","\n","<br>\n","\n","Let's suppose $X$ has four possible values. <br>\n","And I aussme that I've arranged my sample space such that they're kind of arranged by column. <br><br>\n","\n","\n","#### Random Vriables in Pebble World <br><br>\n","\n","Remember, <br>\n","\" a Random Variable is a mapping from the Sample space ! \" <br><br>\n","\n","The sample space is the set of all pebbles to the real line. <br>\n","So suppose I've arranged my pebbles just for simplicity in that picture. <br>\n","Assuming $X$ can only take on these four possible values. <br><br>\n","\n","For example, <br>\n","Each of these pebbles in the first column get mapped $X=0$. <br>\n","So that means you have this random experiment, if the outcome is one of these four pebbles, then we say $X=0$. <br><br>\n","\n","That's what a Random Variable is. <br>\n","Any discrete random variable conceptually looks something like this. \n","\n","<br><br><br>\n","\n","\n","\n","### Two ways of mean average in a pebble world <br><br>\n","\n","\n","Let's make an observation about how we can do our summation. <br><br>\n","\n","$\\mathbb{E}(X)$ <br>\n","We defined it to be the sum over all values $x$ of $x$ times the PMF. <br>\n","$\\mathbb{E}(X) \\quad = \\quad \\displaystyle \\sum_x x P(X=x)$ <br><br>\n","\n","But notice that <br>\n","we could do this a different way instead ! <br>\n","That is to sum not over values $x$ of the Random Variable, but sum over all pebbles $s$. <br><br>\n","\n","I'm thinking the sample space as a capital $S$, <br>\n","and each individual pebbles as a lower case $s$. <br>\n","$\\begin{align} \n","\\mathbb{E}(X) \\quad &= \\quad \\displaystyle \\sum_x x P(X=x) &&\\text{grouped one} \\\\\n","&= \\quad \\sum_x X(s)P(\\{s\\}) &&\\text{ungrouped one} \\\\\n","&&&...\\text{ remember $X$ is a function} \\\\\n","&&&...\\text{ $P(s)$ is a mass of the pebble $s$ in the sample space $S$}\n","\\end{align}$ <br>\n","\n",">Remember, <br>\n",">We have different pebbles whose total mass equals 1, in the pebble world. <br>\n",">So $P(\\{s\\})$ is the mass of pebble $s$.\n","\n",">Notice, <br>\n",">That's obvious once we've stared at this enough to understand what we're doing here. <br>\n",">It's the same thing as doing the average in two different ways like we did last time.\n","\n","<br>\n","\n","So what we're doing with this ( sum over all pebbles ) is <br>\n","just taking each pebble as a sign of certain value and just averaging them all together. <br><br>\n","\n","Of course it's a weighted average <br>\n","cuz some of the pebbles have higher probability possibly. <br><br><br>\n","\n","\n","On the other hand, <br>\n","we could first group together all the pebbles that get assigned $x=0$, $X=1$, and so on. Group all this together. <br>\n","And we treat them as one of big one mass of pebbles with $X=n$, kind of like clumped or glued together. <br><br>\n","\n","That's exactly what the first average definition is saying to do. <br>\n","we grouped them all together $x$, and then give it a larger mass $P(X=x)$. ( as if it's a super pebble ) <br>\n","\n",">Just by thinking og how we could do averages by grouping together terms that have the same value.\n","\n","<br><br>\n","\n","\n","So that's true for ANY RANDOM VARIABLE $X$. <br>\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-spQNxvpQl1m"},"source":["### Proof of Linearity in the discrete case ( continuous case is the same )\n","<br><br>\n","\n",">Recall <br>\n",">$\\; \\mathbb{E}(X) = \\displaystyle \\sum_x xP(X=x) = \\sum_s X(s)P(\\{s\\})$\n","\n","<br>\n","\n","When $\\; T = X+Y$ <br><br>\n","\n","$\\begin{align}\n","\\mathbb{E}(T) \\quad &= \\quad \\displaystyle \\sum_s (X+Y)(s) P(\\{s\\}) \\\\\n","&= \\quad \\sum_s \\big( X(s)+Y(s) \\big) P(\\{s\\}) &&\\text{... by the definition of adding function} \\\\\n","&&& \\text{... adding two function means that you compute both functions and add them} \\\\\n","&= \\quad \\sum_s X(s) P(\\{s\\}) \\sum_s Y(s) P(\\{s\\}) &&\\text{... Now everything is summbed over the same $s$ (pebbles) !} \\\\\n","&&&\\text{( not each over $x$ and $y$ )} \\\\\n","&= \\mathbb{E}(X) + \\mathbb{E}(Y) &&\\text{... according to $\\; \\mathbb{E}(X) = \\displaystyle \\sum_x xP(X=x) = \\sum_s X(s)P(\\{s\\})$}\n","\\end{align}$ <br><br>\n","\n","\n","This is the end of the proof of linearity ( of average ) <br><br>\n","\n","It's almost immediate once you see that <br>\n","you can write the expected value as the sum of pebbles $\\mathbb{E}(X) = \\sum_s X(s)P(\\{s\\})$, <br>\n","rather than in this original form $\\mathbb{E}(X) = \\sum_x xP(X=x)$\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"U6mFqtZKv44-"},"source":["### Another fact about linearity <br><br>\n","\n","\n","There was one other fact about linearity. <br>\n","It is that you can multiply by constants. <br><br>\n","\n","That's the other part of Linearity. <br>\n","And we just use the same thing again. <br><br>\n","\n","$\\mathbb{E}(cX) \\quad = \\quad c \\mathbb{E}(X) \\qquad \\text{, if $c$ is constant}$ <br><br>\n","\n","This one is more intuitive. <br>\n","It is just saying that the expected value of 2 times $X$ is twice the expected balue of $X$. <br><br>\n","\n","By the way actually, this one seems pretty simple, <br>\n","but this also helps explain some of the intuition for \" Why is Linearity ture even if $X$ and $Y$ are dependent ! \" \n","\n","<br><br><br>\n","\n","\n","\n","#### Extreme case of dependence <br><br>\n","\n","\n",">Remember, <br>\n",">it's very useful to look at extreme cases.\n","\n","<br>\n","\n","\" What if $X = Y$ ? \" <br><br>\n","\n","\n","Then, <br>\n","$\\begin{align} \n","\\mathbb{E}(X+Y) &= \\mathbb{E}(2X) \\\\\n","&= 2\\mathbb{E}(X) \\\\\n","&= \\mathbb{E}(X) + \\mathbb{E}(Y)\n","\\end{align}$ <br><br>\n","\n","That's not a proof of Linearity. <br>\n","That's just checking it's truth in the extreme form of dependence. <br>\n","( In the case of independence, it's intuitively true )\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"v0VZD8xp3peJ"},"source":["### Example of Linearity <br><br>\n","\n","Let's do some more examples of <br>\n","\" How we would use the Linearity \". \n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SRZ22_I1-3gO"},"source":["#### Important distributions in Statistics <br><br>\n","\n","\n","We're gonna have a list of 13 or 14 extremely important distributions in Statistics. <br><br>\n","\n","So you might ask ... <br>\n","\" What makes a distribution important ? \" <br><br>\n","\n","\n","Because there're infinitely made distributions we could write down <br>\n","just choose whatever PMF we want or create any CDF we want. <br><br>\n","\n","\n","The reason is that the most important ones all have nice useful stories behind them !  <br>\n","\n",">So we saw the story of the Binomial, the story of the Geometric, the story of the Hypergeometrix, <br>\n",">and there's just a few more of those that we're gonna need soon.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zYJOW0k-6CFk"},"source":["### Negative Binomial <br><br>\n","\n","\n","Actually, <br>\n","The Negative Binomial is a generalization of the geometric distribution. <br>\n","\n",">Don't be fooled by the name. <br>\n",">It's actually not netgative and not a binomial ! <br>\n",">( Just standard terminology )\n","\n","<br>\n","\n","  - Parameter <br>\n","  So it has two parameters of $r$ and $p$. <br><br>\n","\n","And the story is that like the Grometric doing Bernoulli trials over and over again <br>\n","until getting success for the first time then we coutned the number of failures. <br>\n","The Negative Binomial is the extension to the case where you want $r$ successes. <br><br>\n","\n","  - Story <br>\n","  We have independent $Bern(p)$ trials $\\qquad$ ( like with the Geometric ) <br>\n","  And we want to know number of $\\#$failures before the $r$th success. <br>\n","\n","  >Keep trying and trying unril you have $r$th success, and then count the number of failures. <br>\n","  >$\\Rightarrow \\quad$ That would be called a Negative Binomial\n","\n","<br><br>\n","\n","\n","Let's find PMF. <br>\n","Let's find the expected value.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LtEpAe03uzNx"},"source":["### PMF of Negative Binomial <br><br>\n","\n","\n","To find the PMF, <br>\n","we just need to draw a little picture for ourselves, and think about the pircture. <br><br>\n","\n","So the picture is just a sequence. <br>\n","Let's write $1$ for success and $0$ for failure. \n","\n","<br><br>\n","\n","\n","#### Example <br><br>\n","\n","$100010010000100\\color{brown}{1}$ <br>\n","Let's say $r=5$ <br><br>\n","\n","$P(X=n)$ <br>\n","We want to the probability that $X=n$ <br>\n","if $X$ is Nagative Binomial. <br><br>\n","\n","$X=n$ would mean that there are n failures. $\\quad$ ( in this example $n=11$ ) <br>\n","$p$ is whatever it is. We don't have to worry about $p$ right now. <br><br><br>\n","\n","\n","Notice <br>\n","one simple but important fact about this sequence that ends in a $\\color{brown}{1}$ $\\quad$ ( that's the $r$th success ) <br>\n","That could not have been a $0$. $\\quad$ ( That's the one thing I know for sure ) <br><br>\n","\n","Let's consider the rest of this sutff. $\\quad$ ( front of $\\color{brown}{1}$ ) <br>\n","If I permute these $0$s and $1$s, would that still be valid ? <br>\n","It didn't matter, right ? <br><br>\n","\n","Permute these however we eant, it's not gonna change things. <br>\n","It's not gonna change the probability, <br>\n","It's not gonna change whether it's valid or not. <br><br><br>\n","\n","\n","So amongst these earlier terms, <br>\n","we know there are 4 successes and 11 fialures. But those can be in any order. <br><br>\n","\n","And those are the only possibilities ! <br>\n","We need to have the $r$th success, <br>\n","and then the earlier stuff. <br>\n",">we already know how many $0$s, how many $1$s it could be, and they could be in any order.\n","\n","<br><br>\n","\n","\n","PMF <br>\n","Now we can immediately write down the PMF. <br><br>\n","\n","The porobability of the particular sequence, <br>\n","then we multiply it by how many possible ways are there to reagrrange it. <br><br>\n","\n","  - Probability of the particular sequence <br>\n","  The one particular sequence we wrote for example had $r$ successes, and each success has probability $p$. <br>\n","  $\\rightarrow \\quad p^r$ <br>\n","  And there are $n$ failures. <br>\n","  $\\rightarrow \\quad (1-p)^n$ <br><br>\n","\n","  - How many ways are to rearrange it <br>\n","  The ending $\\color{brown}{1}$ has to stay at the finish, but the ones before it could be in any order. <br>\n","  So all we have to do is select the locations of the $0$s, or equivalently select the locations of the $1$s. <br>\n","  $\\rightarrow \\quad \\binom{n+r+1}{r-1}$ <br><br><br>\n","\n","\n","\n","So the PMF is ... <br><br>\n","\n","  - $P(X=n) \\quad = \\quad \\displaystyle \\binom{n+r+1}{r-1} p^r (1-p)^n \\qquad \\text{, for} \\quad n=0,1,2,...$ <br><br>\n","\n",">The reason it has the name \" Negative Binomial \" <br>\n",">is just that in combinatorics this kind of series is closely related to what we get if we did the Binomial theorem.\n","\n",">The Binomial Theorem (이항정리) is $(a+b)$ to a positive power $n$ <br>\n","$(a+b)^n \\quad = \\quad a^n + _nC_1 a^{n-1}b + _nC_2 a^{n-2}b^2 + \\cdots + _nC_k a^{n-k}b^k + \\cdots + b_n$ <br><br>\n",">\n",">If we expand $(a+b)$ to a negative power, <br>\n",">we get something closely related to this ! <br>\n",">( So it's lie a negative power in the binomial theorem ) \n","\n","<br><br>\n","\n","\n","We don't need to worry about the eymology of the term. <br><br>\n","\n","For our purposes <br>\n","what we care about is the story <br>\n","and we care about the PMF. <br>\n","and we care about the mean ( average ) next.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GhncvrhmCotS"},"source":["### Expected value of Negative Binomial <br><br>\n","\n","\n","We have the PMF. <br>\n","$\\rightarrow \\quad$ So in principle, we can compute the mean <br>\n","$\\qquad$ by taking the sum of PMF $P(X=n)$ over all $n$. <br><br>\n","\n",">$P(X=n) \\quad = \\quad \\displaystyle \\binom{n+r+1}{r-1} p^r (1-p)^n \\qquad \\text{, for} \\quad n=0,1,2,...$\n","\n","<br>\n","\n","That looks pretty nasty. <br>\n","So let's try to think of a better way to do it. <br><br>\n","\n","\n",">Think simple and extreme case. <br><br>\n",">\n",">If $r=1 \\quad$ , It's just a geometric distribution. <br>\n",">$\\qquad \\qquad \\;$ That $Geom(p)$ has expected value $\\mathbb{E}(X) = \\frac{q}{p} \\qquad$ ( So we know the case $r=1$ already ) <br><br>\n",">\n",">If $r=2 \\quad$ , We're waiting and waiting until we have 2 successes. <br>\n",">$\\qquad \\qquad \\;$ a natural way to do that <br>\n",">$\\qquad \\qquad \\;$ First we wait for the first success, and then we wait for the second success. <br><br>\n",">\n",">In general $\\;$ , If we want to wait for $r$ successes, <br>\n",">$\\qquad \\qquad \\;$ first wait for the first success, then wait for the second success, and so on ... <br>\n",">$\\qquad \\qquad \\;$ So therefore we can interpret this as <br>\n",">$\\qquad \\qquad \\; \\mathbb{E}(X_1 + X_2 + ... + X_r) \\qquad , \\text{where $X_j$ is #failures between the $j_{-1}$th and the $j$th successes}$\n","\n","<br><br>\n","\n","\n","$\\mathbb{E}(X_1 + X_2 + ... + X_r) \\qquad , \\text{where $X_j$ is #failures between the $j_{-1}$th and the $j$th successes}$ <br><br>\n","\n","We already know the distribution of $X_j$. <br>\n","$X_j$ is just follow Geometric distribution ! <br>\n","$X_j \\sim Geom(p)$ <br><br>\n","\n","In this case, the extras are even independent. <br>\n","( but we deon't even need that for this calculation (with linearity) ) <br><br>\n","\n","$\\begin{align}\n","\\mathbb{E}(X) \\quad &= \\mathbb{E}(X_1 + X_2 + ... + X_r) \\\\\n","&= \\mathbb{E}(X_1) + \\mathbb{E}(X_2) + ... + \\mathbb{E}(X_r) &&\\text{... by Linearity} \\\\\n","&= r \\frac{q}{p} &&\\text{... by the expected value of Geomatric R.V.s}\n","\\end{align}$\n","\n","<br>\n","\n",">We'll wait for the first success <br>\n",">and then the additional for the the scond, <br>\n",">and then the additional for the third, <br>\n",">... <br><br>\n",">\n",">Each one of those is Geometric $p$ ! <br>\n",">Each one of those $q/p$, and there's $r$ of them.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zxKEl-giH_nI"},"source":["### The first success distribution <br><br>\n","\n",">little bit more about the Geometric thing. <br>\n",">The First Success Distribution\n","\n","<br><br>\n","\n","Let's let $X$ is ... <br>\n","$\\begin{align}\n","X \\sim FS(p) &&&\\text{... $FS(p)$ means that the first success distribution of (with) $p$} \\\\\n","&&&\\text{... That's the time until 1st success. $\\quad$ ( we're going to have these Bernoulli $p$ trials )} \\\\\n","&&&\\text{... This one is counting the success.}\n","\\end{align}$ <br><br>\n","\n",">If we just write $Geom(p)$ Geometric, we're not counting the success. $\\quad$ ( We're counting the failures ? ) <br>\n",">If we write $FS(p)$, then we're counting the success.\n","\n","<br><br><br>\n","\n","If we wanted to deal with this problem, <br>\n","we don't need to go through a new theory. <br><br>\n","\n","All we have to do is that, <br>\n","\" Let $Y=X-1 \\quad$ , then the $Y \\sim Geom(p)$ ! \" $\\qquad$ ... by our definition of Geometric dist. <br><br>\n","\n","I subtracted $1$ from $X$, <br>\n","just because I don't want to count the success. That's all. <br><br>\n","\n","And so then, <br>\n","It's easy to relate the two PMF of $X$ and $Y$. $\\qquad$ ... by the Linearity (?) <br><br>\n","\n","If we want the expected value of $X$, <br><br>\n","\n","$\\begin{align}\n","\\mathbb{E}(X) \\quad &= \\mathbb{E}(Y) + 1 \\\\\n","&= \\frac{q}{p} + 1 \\\\\n","&= \\frac{1}{p}\n","\\end{align}$\n","\n","<br><br>\n","\n","\n","#### Intuitive thinking <br><br>\n","\n","Thjis is pretty intuitive when you think about it. <br><br>\n","\n","Suppose that our probability of success is 1 in 10. <br>\n","It says on average it would take 10 trials to get the first sucess, if you include that success. <br><br>\n","\n","So that kind of makes sense, if you secceed 1 in 10 times. <br>\n","It means it's gonna take about 10 trials.\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kzyWIqetTK65"},"source":["### Example1 $\\quad$ with expectation <br><br>\n","\n","\n","#### Problem $\\quad : \\;$ Putnam math <br><br>\n","\n","There's a random permutation of $1,2,...n \\qquad \\qquad$ where $n \\geq 2$<br>\n","\n",">Random means here that all permutation are equally likely. <br>\n",">So we take the numbers $1$ through $n$, and permute them in any order.\n","\n","<br>\n","\n","Find the expected $\\#$ of local maxima <br>\n","\n",">Local Maxima <br>\n",">means the any number that's bigger than it's neighbors. <br>\n",">EX. $\\quad 3 \\; 2 \\; 1 \\; 4 \\; 7 \\; 5 \\; 6$ <br>\n",">In these numbers, the local maxima is $7$ which is between $4$ and $5$.\n","\n","<br><br>\n","\n","\n","#### Solution <br><br>\n","\n","  1. First, <br>\n","  we need some Indicator Random Variables ! <br><br>\n","\n","  Let $I_j$ be Indicator R.V. $\\qquad$ ( for a position $j$ being a local maxima ) <br>\n","\n","  >That's and event ! <br>\n","  >the event that either it does or does not have a local maxima at that position.\n","\n","<br><br>\n","\n","  2. Then, <br>\n","  the sum of these $I_j$s is the number $\\#$ of local maxima ! $\\qquad$ ( That's what we're trying to count ) <br><br>\n","\n","  So what we're interested in is the expected value $\\mathbb{E}(I_1 + I_2 + ... + I_n)$ <br><br>\n","\n","  $\\begin{align}\n","\\mathbb{E}(I_1 + I_2 + ... + I_n) \\quad &= \\mathbb{E}(I_1) + \\mathbb{E}(I_2) + ... \\mathbb{E}(I_n) &&\\text{... by Linearity} \\\\\n","&= \\frac{n-2}{3} + \\frac{2}{2} &&\\text{... by Symmetry} \\\\\n","&= \\frac{n+1}{3}\n","\\end{align}$ <br><br><br>\n","\n","\n","\n","  - Fundametal bridge <br>\n","The expected value ($\\mathbb{E}(I_k)$) of an Indicator R.V. ($I_k$) is the probability of the event. <br><br>\n","\n","\n","  - Account for the probability of the event $I_j$. <br>\n","Because the permutation is equally likely, we focus on the three numbers related to the number $\\#$ of local maxima. <br>\n","( a center number and two numbers next to that ) <br><br>\n","\n","  >For example, <br>\n","  >we focus on the three numbers $4, 7, 5$, they're equally likely to be in any of the six possible orders. <br><br>\n","  >\n","  >The one way to think of it is ... <br>\n","  >the number of permutation of these three numbers is $3!$ (3 factorial), <br>\n","  >and the number of $j$th position is local maxima is $2 \\quad$ ( just $\\{4,7,5\\}$ and $\\{5,7,4\\}$ cases ) <br><br>\n","  >\n","  >Or even better way to think of it is ... <br>\n","  >In these three numbers, the largest number among those is equally likely to be in any of those three positions. <br>\n","  >So there's an $1/3$ chance that the biggest number ($7$) is in the middle. <br><br>\n","  >\n","\n","  >Inccorect argument <br>\n","  >A common mistake with this, <br>\n","  >is what to say the answer is $1/4$ because there's a $1/2$ chance that $7 \\geq 4$ and there's a $1/2$ chance that $7 \\geq 5$. <br>\n","  >( and so a lot of people would say like this ... ) <br><br>\n","  >\n","  >Not independent ! <br>\n","  >$7 \\geq 4$ is not independent of $7 \\geq 5$ ! <br>\n","  >( if they were independent, it would be $1/4$ )\n","\n","<br>\n","\n","  - Be carefule to distinguish between the intermediate points and the endpoints. <br>\n","  The two positions of the endpoints have the probability of $1/2$ that an event happens.\n","\n","<br><br><br>\n","\n","\n","\n",">This answer grows linearly, $(n+1)/3$. <br>\n",">And it goes to infinity as $n$ goes to infinity. It would be a little bit weird if they expect a number of max in infinity ! <br><br>\n",">\n",">But, to solve this really hard problem, <br>\n",">We just use combinations, indicators, linearity, symmetry !\n","\n","<br><br><br>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4fY-vNjglj9c"},"source":["### Example 2 $\\quad$ with expectation <br><br>\n","\n","\n","#### St.Petersburg Paradox <br><br>\n","\n","\n","#### Problem <br><br>\n","\n","They're going to flip a fair coin over and over again, until the coin leands heads for the first time. <br>\n","And if the coin lands heads on the first trial, then you'll get \\$$2$. <br>\n","If it's the second trial for the first outcome of heads you'll get \\$$4$. <br>\n","If it's the third trial for the first outcome of heads you'll get \\$$8$. <br>\n","... <br><br>\n","\n","We get \\$$2^X \\quad$ , where $X$ is $\\#$ flips of the coin until first until the first head(H) ( including the success (H) ) <br>\n","\n",">So that's what we call the First Sucess distribution before. <br>\n",">So it's a Geometric Distribution ( +1, because we're counting that success(H) )\n","\n","<br>\n","\n","The question is how much should we be willing to pay to play this game. <br>\n","( What's fair price that would make the ecpected value $0$ ? ) <br><br><br>\n","\n","\n","\n","#### Solution <br><br>\n","\n","When $Y=2^X \\qquad \\qquad$ ( That's your payoff ) <br>\n","Find $\\;\\; \\mathbb{E}(Y)$ <br><br>\n","\n","$\\begin{align}\n","\\mathbb{E}(Y) &=\\displaystyle \\sum_{k=1}^{\\infty} 2^k \\frac{1}{2^k} &&\\text{... by the definition of expecteation $\\quad$ : sum of the values * prob.s} \\\\\n","&&&\\text{... the value is $2^k$, the probability of that is 1/2 by the fair coin flipping} \\\\\n","&= \\displaystyle \\sum_{k=1}^{\\infty} 1 \\\\\n","&= \\infty\n","\\end{align}$ <br><br>\n","\n","\n","$\\rightarrow \\quad$ So according to this calculation, <br>\n","$\\qquad$ you should be willing to pay an infinite amount of money to play this game !\n","\n","<br><br><br>\n","\n","\n","\n","#### Problem twist <br><br>\n","\n","\n","If our budget is bound at \\$$2^{40}$, <br>\n","How much should we be willing to pay to play this game ?? <br><br>\n","\n","Then, what we expecte to get is ... <br><br>\n","\n","$\\begin{align}\n","\\mathbb{E}(Y) &=\\displaystyle \\sum_{k=1}^{40} 2^k \\frac{1}{2^k} \\\\\n","&= \\displaystyle \\sum_{k=1}^{40} 1 \\\\\n","&= 40\n","\\end{align}$ <br><br>\n","\n","We only expect to get 40$ ! <br>\n","That's a big difference. even by imposing a bound of a trillion dollars. <br>\n","( It's a astronomically huge amount of money ! )\n","\n","<br><br><br>\n","\n","\n","\n","#### Caution <br><br>\n","\n","$\\infty = \\mathbb{E}(2^X) \\quad \\not= \\quad 2^{\\mathbb{E}(X)} = 2^2 \\qquad$ ... by the expectation of the First Success dist. is $1/p$ <br><br>\n","\n","\" This is not like Linearity ! \"\n","\n","\n","<br><br><br>\n","\n","\n","\n","\n","\n","\n","\n"]}]}